{"pages":[],"posts":[{"title":"YOLO3 目标检测与性能评价 Keras源码分析","text":"使用 YOLO3配置 yolo.py修改 yolo.py 的 _defaults，主要是把自己训练好的权重文件路径、锚框文件路径、类别文件路径配置上。 _defaults = { \"model_path\": 'trained_weights_final.h5', \"anchors_path\": 'yolo_anchors.txt', \"classes_path\": 'voc_classes.txt', \"score\" : 0.3, \"iou\" : 0.45, \"model_image_size\" : (416, 416), \"gpu_num\" : 1, } 运行 yolo_video.pypython yolo_video.py --image 对单张图片进行测试，python yolo_video.py --video 对视频进行测试。 图片检测源码分析基础函数letterbox_image位于 \\utils.py def letterbox_image(image, size): '''resize image with unchanged aspect ratio using padding''' iw, ih = image.size w, h = size scale = min(w/iw, h/ih) nw = int(iw*scale) nh = int(ih*scale) image = image.resize((nw,nh), Image.BICUBIC) new_image = Image.new('RGB', size, (128,128,128)) new_image.paste(image, ((w-nw)//2, (h-nh)//2)) return new_image 使用填充调整图像大小，保持纵横比不变 参数： image： Image 对象 size：目标大小。 返回： 返回已经调整过大小的新的 Image 对象 执行过程： 获取原图的宽和高，iw, ih 获取目标的宽和高，w, h 取 w/iw, h/ih 中最小的比例作为缩放比例 scale 按照缩放比例计算新的宽和高，nw = iw*scale, nh=ih*scale 将 Image 对象按照新的宽高进行调整，仍命名为 image 创建一个尺寸为 size 颜色为灰色的 Image 对象，命名为 new_image 将 image 贴到 new_image 中间 返回 new_image YOLO.__init__(self, **kwargs)位于 \\yolo.py def __init__(self, **kwargs): self.__dict__.update(self._defaults) # set up default values self.__dict__.update(kwargs) # and update with user overrides self.class_names = self._get_class() self.anchors = self._get_anchors() self.sess = K.get_session() self.boxes, self.scores, self.classes = self.generate() YOLO 类的初始化，主要完成以下任务： 通过 self._get_anchors() 获得类别名 通过 self._get_anchors() 获取锚框数组，并记为 self.anchors 通过 K.get_session() 获得 session，记为 self.sess 通过 self.generate() 得到 self.boxes, self.scores, self.classes，这一步并不会得到具体的值，只是在 YOLO 网络模型的运算图后衔接 yolo_eval 定义的运算图。 YOLO._get_class()def _get_class(self): classes_path = os.path.expanduser(self.classes_path) with open(classes_path) as f: class_names = f.readlines() class_names = [c.strip() for c in class_names] return class_names 通过 self.classes_path 的路径，读取类别文件，获得类别名数组 YOLO._get_anchors()def _get_anchors(self): anchors_path = os.path.expanduser(self.anchors_path) with open(anchors_path) as f: anchors = f.readline() anchors = [float(x) for x in anchors.split(',')] return np.array(anchors).reshape(-1, 2) 通过 self.anchors_path) 路径，读取锚框文件，获得锚框数组 YOLO.generate()def generate(self): model_path = os.path.expanduser(self.model_path) assert model_path.endswith('.h5'), 'Keras model or weights must be a .h5 file.' # Load model, or construct model and load weights. num_anchors = len(self.anchors) num_classes = len(self.class_names) is_tiny_version = num_anchors==6 # default setting try: self.yolo_model = load_model(model_path, compile=False) except: self.yolo_model = tiny_yolo_body(Input(shape=(None,None,3)), num_anchors//2, num_classes) \\ if is_tiny_version else yolo_body(Input(shape=(None,None,3)), num_anchors//3, num_classes) self.yolo_model.load_weights(self.model_path) # make sure model, anchors and classes match else: assert self.yolo_model.layers[-1].output_shape[-1] == \\ num_anchors/len(self.yolo_model.output) * (num_classes + 5), \\ 'Mismatch between model and given anchor and class sizes' print('{} model, anchors, and classes loaded.'.format(model_path)) # Generate colors for drawing bounding boxes. hsv_tuples = [(x / len(self.class_names), 1., 1.) for x in range(len(self.class_names))] self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples)) self.colors = list( map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), self.colors)) np.random.seed(10101) # Fixed seed for consistent colors across runs. np.random.shuffle(self.colors) # Shuffle colors to decorrelate adjacent classes. np.random.seed(None) # Reset seed to default. # Generate output tensor targets for filtered bounding boxes. self.input_image_shape = K.placeholder(shape=(2, )) if self.gpu_num&gt;=2: self.yolo_model = multi_gpu_model(self.yolo_model, gpus=self.gpu_num) boxes, scores, classes = yolo_eval(self.yolo_model.output, self.anchors, len(self.class_names), self.input_image_shape, score_threshold=self.score, iou_threshold=self.iou) return boxes, scores, classes generate() 函数主要干了三件事，加载模型、生成类别颜色框，完善模型的运算图。 执行过程： model_path = os.path.expanduser(self.model_path)assert model_path.endswith('.h5'), 'Keras model or weights must be a .h5 file.'# Load model, or construct model and load weights.num_anchors = len(self.anchors)num_classes = len(self.class_names)is_tiny_version = num_anchors==6 # default settingtry: self.yolo_model = load_model(model_path, compile=False)except: self.yolo_model = tiny_yolo_body(Input(shape=(None,None,3)), num_anchors//2, num_classes) \\ if is_tiny_version else yolo_body(Input(shape=(None,None,3)), num_anchors//3, num_classes) self.yolo_model.load_weights(self.model_path) # make sure model, anchors and classes matchelse: assert self.yolo_model.layers[-1].output_shape[-1] == \\ num_anchors/len(self.yolo_model.output) * (num_classes + 5), \\ 'Mismatch between model and given anchor and class sizes' 通过锚框数目判断是否为 tiny 版本 尝试直接加载模型，如果 .h5 文件本身带有模型结构的话。 如果 .h5 文件本身不带有模型结构，就先根据是否是 tiny 版本，创造对应的模型，然后再加载权重 最终 self.yolo_model 储存着 yolo3 的模型 # Generate colors for drawing bounding boxes.hsv_tuples = [(x / len(self.class_names), 1., 1.) for x in range(len(self.class_names))]self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))self.colors = list( map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), self.colors))np.random.seed(10101) # Fixed seed for consistent colors across runs.np.random.shuffle(self.colors) # Shuffle colors to decorrelate adjacent classes.np.random.seed(None) # Reset seed to default. 随机生成不同类别的预测框的颜色 # Generate output tensor targets for filtered bounding boxes.self.input_image_shape = K.placeholder(shape=(2, ))if self.gpu_num&gt;=2: self.yolo_model = multi_gpu_model(self.yolo_model, gpus=self.gpu_num)boxes, scores, classes = yolo_eval(self.yolo_model.output, self.anchors, len(self.class_names), self.input_image_shape, score_threshold=self.score, iou_threshold=self.iou)return boxes, scores, classes 判断 GPU 数目，如果 GPU 数目大于 2 则将模型升级为多 GPU 模型 在 self.yolo_model.output 衔接 yolo_eval 定义的运算图， 通过 yolo_eval 可过滤大部分无效的预测框。 yolo_eval()def yolo_eval(yolo_outputs, anchors, num_classes, image_shape, max_boxes=20, score_threshold=.6, iou_threshold=.5): \"\"\"Evaluate YOLO model on given input and return filtered boxes.\"\"\" num_layers = len(yolo_outputs) anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]] # default setting input_shape = K.shape(yolo_outputs[0])[1:3] * 32 boxes = [] box_scores = [] for l in range(num_layers): _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[l], anchors[anchor_mask[l]], num_classes, input_shape, image_shape) boxes.append(_boxes) box_scores.append(_box_scores) boxes = K.concatenate(boxes, axis=0) box_scores = K.concatenate(box_scores, axis=0) mask = box_scores &gt;= score_threshold max_boxes_tensor = K.constant(max_boxes, dtype='int32') boxes_ = [] scores_ = [] classes_ = [] for c in range(num_classes): # TODO: use keras backend instead of tf. class_boxes = tf.boolean_mask(boxes, mask[:, c]) class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c]) nms_index = tf.image.non_max_suppression( class_boxes, class_box_scores, max_boxes_tensor, iou_threshold=iou_threshold) class_boxes = K.gather(class_boxes, nms_index) class_box_scores = K.gather(class_box_scores, nms_index) classes = K.ones_like(class_box_scores, 'int32') * c boxes_.append(class_boxes) scores_.append(class_box_scores) classes_.append(classes) boxes_ = K.concatenate(boxes_, axis=0) scores_ = K.concatenate(scores_, axis=0) classes_ = K.concatenate(classes_, axis=0) return boxes_, scores_, classes_ 参数： yolo_outputs：yolo_body 模型的输出 anchors：锚框数组 num_classes：类别数目 image_shape： 图片尺寸 max_boxes：一张图片中，最多出现的预测框数目，默认 20 score_threshold：预测框分数阈值，默认 0.6，预测框分数指预测框置信度与类别可能性的积。 iou_threshold：IOU 阈值，默认 0.5 返回： boxex_ ：预测框列表，每个预测框用绝对四边坐标表示。 scores_：预测框分数列表，上述预测框对应于的分数。 classes_：类别列表，上述预测框对应的类别独热码。 执行过程： num_layers = len(yolo_outputs)anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]] # default settinginput_shape = K.shape(yolo_outputs[0])[1:3] * 32boxes = []box_scores = []for l in range(num_layers): _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[l], anchors[anchor_mask[l]], num_classes, input_shape, image_shape) boxes.append(_boxes) box_scores.append(_box_scores)boxes = K.concatenate(boxes, axis=0)box_scores = K.concatenate(box_scores, axis=0) 先获取 yolo_outputs 中有多少特征图的输出，记为 num_layers anchor_mask 为锚框掩码，为每一个特征图分配锚框列表中的锚框 input_shape 是指 yolo_body 的输入尺寸，用第一个特征图的尺寸乘 32 即可 创建预测框列表 boxes 、预测框分数列表 box_scores 循环遍历每个输出的特征图，操作是假设建立在第 l 号特征图上 通过 yolo_boxes_and_scores 函数，l 号特征图中的预测框信息，和预测框分数信息。 将分离出来的盒子信息追加到预测框列表 boxes ，预测框分数追加到预测框分数列表 box_scores 按第一维度（不同特征图）连接 boxes 和 box_scores 中的张量，也就是说将多特征图的结果组合成在一起。 mask = box_scores &gt;= score_thresholdmax_boxes_tensor = K.constant(max_boxes, dtype='int32')boxes_ = []scores_ = []classes_ = []for c in range(num_classes): # TODO: use keras backend instead of tf. class_boxes = tf.boolean_mask(boxes, mask[:, c]) class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c]) nms_index = tf.image.non_max_suppression( class_boxes, class_box_scores, max_boxes_tensor, iou_threshold=iou_threshold) class_boxes = K.gather(class_boxes, nms_index) class_box_scores = K.gather(class_box_scores, nms_index) classes = K.ones_like(class_box_scores, 'int32') * c boxes_.append(class_boxes) scores_.append(class_box_scores) classes_.append(classes)boxes_ = K.concatenate(boxes_, axis=0)scores_ = K.concatenate(scores_, axis=0)classes_ = K.concatenate(classes_, axis=0)return boxes_, scores_, classes_ 设置预测框分数掩码 mask 要求预测框分数大于等于预测框分数阈值 score_threshold 设置代表最大预测框数目的张量，max_boxes_tensor 建立最终返回的预测框列表 boxes_ ，最终返回的预测框分数列表 scores_，最终返回的类别列表 classes_ 遍历每一个类别，以下操作第 c 号类别 使用 tf.boolean_mask 从 boxes 中提取预测框分数大于阈值且预测类别为 c 号类的预测框，将结果记为 class_boxes 使用 tf.boolean_mask 从 c 号类的预测框分数列表 box_scores[:, c] 中，提取预测框分数大于阈值且预测类别为 c 号类的分数值，记为 class_box_scores ，显然 class_boxes 与 class_box_scores 具有对应关系。 使用 TensorFlow 的 nms 方法 tf.image.non_max_suppression ，获取 nms 后剩余的预测框的索引 nms_index 。 使用 K.gather 按照索引再提取从 class_boxes 与 class_box_scores 中提取预测框与预测框分数，得到新的 class_boxes 与 class_box_scores 通过 K.ones_like(class_box_scores, 'int32') * c 得到新的类别独热码 class_boxes 、class_box_scores 、classes 分别追加到 boxes_ 、scores_、classes_ 中 最后将 boxes_ 、scores_、classes_ 中每个类别的结果，通过 K.concatenate 合并在一起。 返回 boxes_ 、scores_、classes_ yolo_boxes_and_scoresdef yolo_boxes_and_scores(feats, anchors, num_classes, input_shape, image_shape): '''Process Conv layer output''' box_xy, box_wh, box_confidence, box_class_probs = yolo_head(feats, anchors, num_classes, input_shape) boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape) boxes = K.reshape(boxes, [-1, 4]) box_scores = box_confidence * box_class_probs box_scores = K.reshape(box_scores, [-1, num_classes]) return boxes, box_scores 参数： feats：特征图 anchors：锚框数目 num_classes：类别数目 input_shape：网络输入尺寸 image_shape：实际输入的图片尺寸 返回： 预测框数组 boxes 与预测框分数数组 box_scores 执行过程： 先使用 yolo_head 将特诊图的输出划分为 box_xy, box_wh, box_confidence, box_class_probs ，yolo_head 在前面的博文介绍过 再使用 yolo_correct_boxes 函数，将 box_xy, box_wh 依据网络的输入尺寸和实际图片的尺寸进行修正，并将它们合并在一起。把输出结果记为 boxes ，boxes 内储存着每个预测框在图片上的绝对四边坐标。 对 boxes 的 shape 进行更改，使之变成 [预测框数目，4] box_scores 预测框分数，它值等于置信度 * 属于某个的类别可能性 对 box_scores 的 shape 进行更改，使之变成 [预测框数目，类别数目] 返回 boxes 和 box_scores yolo_correct_boxesdef yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape): '''Get corrected boxes''' box_yx = box_xy[..., ::-1] box_hw = box_wh[..., ::-1] input_shape = K.cast(input_shape, K.dtype(box_yx)) image_shape = K.cast(image_shape, K.dtype(box_yx)) new_shape = K.round(image_shape * K.min(input_shape/image_shape)) offset = (input_shape-new_shape)/2./input_shape scale = input_shape/new_shape box_yx = (box_yx - offset) * scale box_hw *= scale box_mins = box_yx - (box_hw / 2.) box_maxes = box_yx + (box_hw / 2.) boxes = K.concatenate([ box_mins[..., 0:1], # y_min box_mins[..., 1:2], # x_min box_maxes[..., 0:1], # y_max box_maxes[..., 1:2] # x_max ]) # Scale boxes back to original image shape. boxes *= K.concatenate([image_shape, image_shape]) return boxes 参数： box_xy：预测框中心点坐标 box_wh：预测框的宽高信息 input_shape：模型的输入尺寸 image_shape：实际图片尺寸 返回： boxes：shape=[...,4] 4 分别代表 y_min,x_min,y_max,x_max 是在原图上的绝对坐标 执行过程： box_yx = box_xy[..., ::-1]box_hw = box_wh[..., ::-1] 转置最后两个维度 input_shape = K.cast(input_shape, K.dtype(box_yx))image_shape = K.cast(image_shape, K.dtype(box_yx))new_shape = K.round(image_shape * K.min(input_shape/image_shape))offset = (input_shape-new_shape)/2./input_shapescale = input_shape/new_shapebox_yx = (box_yx - offset) * scalebox_hw *= scale 获得输入尺寸张量 input_shape，图片尺寸张量 image_shape 通过 K.min(input_shape/image_shape) 得到 input_shape 与 image_shape 之间的最小比例 image_shape 去乘这个最小比例，结果是将 image_shape 所表示的尺寸进行缩放，这个尺寸恰好能放到 input_shape 尺寸的图片内。 使用 K.round 对上述尺寸进行四舍五入，得到整数的尺寸值，将结果保存到 new_shape 中 计算图像偏移比例 offset 等于（输入尺寸-新尺寸 / 2）/ 输入尺寸 计算缩放比例 scale 等于输入尺寸 / 新尺寸 根据偏移比例和缩放比例，计算新的 box_yx 和 box_hw box_mins = box_yx - (box_hw / 2.)box_maxes = box_yx + (box_hw / 2.)boxes = K.concatenate([ box_mins[..., 0:1], # y_min box_mins[..., 1:2], # x_min box_maxes[..., 0:1], # y_max box_maxes[..., 1:2] # x_max])boxes *= K.concatenate([image_shape, image_shape])return boxes 计算预测框的四边坐标在图中的比例 y_min,x_min,y_max,x_max 将四边坐标比例合并在 boxes 变量内 四边坐标比例乘输入图片尺寸，得到四边坐标在输入的图片中的真实尺寸，结果保存在 boxes 变量内 返回 boxes 实现函数detect_img(yolo)位于 \\yolo_video.py def detect_img(yolo): while True: img = input('Input image filename:') try: image = Image.open(img) except: print('Open Error! Try again!') continue else: r_image = yolo.detect_image(image) r_image.show() yolo.close_session() 参数和返回值： 参数 yolo 是一个 YOLO 实例，YOLO 类被定义在 \\yolo.py 中 执行过程： 通过 input() 内置函数读取图片路径 使用 PIL 的 Image 从这个路径上读取图片到变量，image 调用 YOLO 对象的 detect_image 方法，该方法返回一个 Image 对象 ，记为 r_image 展示 r_image，并关闭 session YOLO.detect_image(self, image)位于 \\yolo.py def detect_image(self, image): start = timer() if self.model_image_size != (None, None): assert self.model_image_size[0]%32 == 0, 'Multiples of 32 required' assert self.model_image_size[1]%32 == 0, 'Multiples of 32 required' boxed_image = letterbox_image(image, tuple(reversed(self.model_image_size))) else: new_image_size = (image.width - (image.width % 32), image.height - (image.height % 32)) boxed_image = letterbox_image(image, new_image_size) image_data = np.array(boxed_image, dtype='float32') print(image_data.shape) image_data /= 255. image_data = np.expand_dims(image_data, 0) # Add batch dimension. out_boxes, out_scores, out_classes = self.sess.run( [self.boxes, self.scores, self.classes], feed_dict={ self.yolo_model.input: image_data, self.input_image_shape: [image.size[1], image.size[0]], K.learning_phase(): 0 }) print('Found {} boxes for {}'.format(len(out_boxes), 'img')) font = ImageFont.truetype(font='font/FiraMono-Medium.otf', size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32')) thickness = (image.size[0] + image.size[1]) // 300 for i, c in reversed(list(enumerate(out_classes))): predicted_class = self.class_names[c] box = out_boxes[i] score = out_scores[i] label = '{} {:.2f}'.format(predicted_class, score) draw = ImageDraw.Draw(image) label_size = draw.textsize(label, font) top, left, bottom, right = box top = max(0, np.floor(top + 0.5).astype('int32')) left = max(0, np.floor(left + 0.5).astype('int32')) bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32')) right = min(image.size[0], np.floor(right + 0.5).astype('int32')) print(label, (left, top), (right, bottom)) if top - label_size[1] &gt;= 0: text_origin = np.array([left, top - label_size[1]]) else: text_origin = np.array([left, top + 1]) # My kingdom for a good redistributable image drawing library. for i in range(thickness): draw.rectangle( [left + i, top + i, right - i, bottom - i], outline=self.colors[c]) draw.rectangle( [tuple(text_origin), tuple(text_origin + label_size)], fill=self.colors[c]) draw.text(text_origin, label, fill=(0, 0, 0), font=font) del draw end = timer() print(end - start) return image 参数和返回值： 参数是一个 Image 对象，返回一个 Image 对象。 执行过程： start = timer() timer() 定义于 from timeit import default_timer as timer 用户获取当前时间，变量 start 将配合变量 end 实现度量单张图片处理所用时长。 if self.model_image_size != (None, None): assert self.model_image_size[0]%32 == 0, 'Multiples of 32 required' assert self.model_image_size[1]%32 == 0, 'Multiples of 32 required' boxed_image = letterbox_image(image, tuple(reversed(self.model_image_size)))else: new_image_size = (image.width - (image.width % 32), image.height - (image.height % 32)) boxed_image = letterbox_image(image, new_image_size)image_data = np.array(boxed_image, dtype='float32') 将从参数传入的 Image 对象重新调整大小，调整到 self.model_image_size 规定的大小，并转换成 np 数组，记为 image_data 。 print(image_data.shape)image_data /= 255.image_data = np.expand_dims(image_data, 0) # Add batch dimension. 对 image_data 进行处理，首先使它的值缩放到 0 到 1 ，其次对它增加批维度 out_boxes, out_scores, out_classes = self.sess.run( [self.boxes, self.scores, self.classes], feed_dict={ self.yolo_model.input: image_data, self.input_image_shape: [image.size[1], image.size[0]], K.learning_phase(): 0 }) 在类初始化阶段，已经调用 self.generate() 得到 self.boxes, self.scores, self.classes 这其实是在原有模型的计算图后，衔接了新的运算图。通过 self.sess.run() 可以得到，当给一下结点规定指定值时： self.yolo_model.input: image_data,self.input_image_shape: [image.size[1], image.size[0]],K.learning_phase(): 0 计算图中 [self.boxes, self.scores, self.classes] 变量的值，将返回值保存为out_boxes, out_scores, out_classes font = ImageFont.truetype(font='font/FiraMono-Medium.otf', size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))thickness = (image.size[0] + image.size[1]) // 300for i, c in reversed(list(enumerate(out_classes))): predicted_class = self.class_names[c] box = out_boxes[i] score = out_scores[i] label = '{} {:.2f}'.format(predicted_class, score) draw = ImageDraw.Draw(image) label_size = draw.textsize(label, font) top, left, bottom, right = box top = max(0, np.floor(top + 0.5).astype('int32')) left = max(0, np.floor(left + 0.5).astype('int32')) bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32')) right = min(image.size[0], np.floor(right + 0.5).astype('int32')) print(label, (left, top), (right, bottom)) if top - label_size[1] &gt;= 0: text_origin = np.array([left, top - label_size[1]]) else: text_origin = np.array([left, top + 1]) # My kingdom for a good redistributable image drawing library. for i in range(thickness): draw.rectangle( [left + i, top + i, right - i, bottom - i], outline=self.colors[c]) draw.rectangle( [tuple(text_origin), tuple(text_origin + label_size)], fill=self.colors[c]) draw.text(text_origin, label, fill=(0, 0, 0), font=font) del draw 根据 out_boxes, out_scores, out_classes 在原图上绘制预测框，绘制完预测框后的图像，保存在 image 中 end = timer()print(end - start)return image 记录结束时间，并打印总耗时 返回 image 变量 视频检测源码分析实现函数detect_video位于 \\yolo.py def detect_video(yolo, video_path, output_path=\"\"): import cv2 vid = cv2.VideoCapture(video_path) if not vid.isOpened(): raise IOError(\"Couldn't open webcam or video\") video_FourCC = int(vid.get(cv2.CAP_PROP_FOURCC)) video_fps = vid.get(cv2.CAP_PROP_FPS) video_size = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))) isOutput = True if output_path != \"\" else False if isOutput: print(\"!!! TYPE:\", type(output_path), type(video_FourCC), type(video_fps), type(video_size)) out = cv2.VideoWriter(output_path, video_FourCC, video_fps, video_size) accum_time = 0 curr_fps = 0 fps = \"FPS: ??\" prev_time = timer() while True: return_value, frame = vid.read() image = Image.fromarray(frame) image = yolo.detect_image(image) result = np.asarray(image) curr_time = timer() exec_time = curr_time - prev_time prev_time = curr_time accum_time = accum_time + exec_time curr_fps = curr_fps + 1 if accum_time &gt; 1: accum_time = accum_time - 1 fps = \"FPS: \" + str(curr_fps) curr_fps = 0 cv2.putText(result, text=fps, org=(3, 15), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.50, color=(255, 0, 0), thickness=2) cv2.namedWindow(\"result\", cv2.WINDOW_NORMAL) cv2.imshow(\"result\", result) if isOutput: out.write(result) if cv2.waitKey(1) &amp; 0xFF == ord('q'): break yolo.close_session() 原理就是用 cv2 截视频里的每一帧，然后调用 image = yolo.detect_image(image) 对每一帧进行识别和输出。 性能评价TP TN FP FN 的定义从样本角度分类，可分为两类： 正例（Positives） 负例（Negatives） 从分类器的结果正确与否，可以为两类： 分类正确（True ） 分类错误（False ） 那么结合上述两种划分，可以得到四类： 分类正确的正例（True Positives，TP） 分类正确的负例（True Negatives，TN） 分类错误的正例（False Positives，FP） 分类错误的负例（False Negatives，FN） 查准率（Precision）查准率（Precision）表示在所有分类为正例的情况下（包括真正例和假反例），分类正确的正例占了多少比例。 Precision=\\frac{TP}{TP+FN}召回率（Recall，又称为 TPR）召回率（Recall, 又称为 TPR）表示在所有正例中（包括真正例和假正例），分类正确的正例占了多少比例。 Recall=\\frac{TP}{TP+FP}交并比（Intersection over Union，IOU）IOU (intersection over union) 为检测结果（预测框）与真实框 Ground Truth 的交集面积比上它们的并集面积。 设 A , B 为表示平面的集合，S 为求面积的函数，则： IOU(A,B)=\\frac{S(A\\cap B)}{S(A\\cup B)}目标检测任务中的 TP FP将预测框以分数为依据，按照某一顺序进行排序。若预测框的分数大于某一阈值（Score threshold），则被视为预测为正例（Positives）。 同时，设立一个 IOU 阈值（IOU threshold），如果一个被视为正例的预测框与真实框的 IOU 大于该 IOU 阈值，则表示这是一个正确的预测，即 TP；小于该阈值，则说明这是一个错误的预测，记为 FP。 如果对于同一个真实框有多个预测框满足 IOU 大于阈值，此时只将 IOU 最大的作为 TP，其余作为FP。 在 yolo3 网络中，预测框的分数，等于预测框的置信度与类别概率的积。 目标检测任务中的查准率（Precision）与召回率（Recall）当给定一组分数阈值和 IOU 阈值，便可以求出一个类别中的 TP 与 FN，TP+FN 等于该类别的预测框总数 。同时 TP+FP 等于该类别的真实框总数，也是已知数据。所以给定两个阈值便可由 $Precision=\\frac{TP}{TP+FN}$ 与 $Recall=\\frac{TP}{TP+FP}$ 求得一组数据 $(r,p)$ 。 AP 与 mAP通常 IOU 阈值不变，被设定为 0.5 ，所以对于某一个类别而言，不同的分数阈值将对应不同的 $(r,p)$ ，给定 n 个分数阈值则可求出 n 个 $(r,p)$ 。将召回率（Recall）作为横轴，查准率（Precision）作为纵轴，n 个 $(r,p)$ 呈现的图像被称为 P-R 图，一个类别的 AP 就是 P-R 图上各点横纵坐标之积的和，为简化运算通常先将 P-R 图变化为单调递减再计算 AP 如何得到 n 个 $(r,p)$ 点 将总数为 N 的预测框按照分数进行降序排序，判断每个预测框是 TP 还是 FP 取前 k 个预测框为一组，求这一组预测框的召回率（Recall）和查准率（Precision），k取 （1，2，…，N），得到 N 个 $(r,p)$ 点 如何计算 P-R 图上的 AP 补全区间，添加 $(0,1),(1,0)$ 两个 $(r,p)$ 点 将 P-R 图像变成单调递减的图像，参考以下代码： for i in range(len(mpre)-2, -1, -1): mpre[i] = max(mpre[i], mpre[i+1]) 即，如果后一项比前一项大，则令前一项等于后一项。通过这种方式保证只能前一项大于等于后一项。 计算 AP，参考以下代码： i_list = []for i in range(1, len(mrec)): if mrec[i] != mrec[i-1]: i_list.append(i)ap = 0.0for i in i_list: ap += ((mrec[i]-mrec[i-1])*mpre[i]) 即，如果rec的某项与前一项相比变化了，则记录该项索引，构建一个索引列表，最后根据索引列表，计算 AP 如何计算 mAP mAP 是所有类别 AP 的平均值。 参考链接 https://blog.csdn.net/qq_35916487/article/details/89076570?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task https://blog.csdn.net/zdh2010xyz/article/details/54293298?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task https://blog.csdn.net/weixin_38106878/article/details/89199961 https://blog.csdn.net/plsong_csdn/article/details/89502117","link":"/2020/04/04/YOLO3%20%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8E%E6%80%A7%E8%83%BD%E8%AF%84%E4%BB%B7%20Keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"YOLO3 参数学习 Keras源码分析","text":"参数学习损失函数Keras 源码中的损失函数： loss(object)=-\\sum_{i=0}^{K\\times K}\\sum_{j=0}^M I_{ij}^{obj} \\cdot (2-w_i\\times h_i) \\cdot [\\hat x_ilog(x_i)+(1-\\hat x_i)log(1-x_i)]-\\\\ \\sum_{i=0}^{K\\times K}\\sum_{j=0}^M I_{ij}^{obj} \\cdot (2-w_i\\times h_i) \\cdot [\\hat y_ilog(y_i)+(1-\\hat y_i)log(1-y_i)]+\\\\ 0.5 \\cdot \\sum_{i=0}^{K\\times K}\\sum_{j=0}^M I_{ij}^{obj} \\cdot (2-w_i\\times h_i) \\cdot [(w_i-\\hat w_i)^2+(h_i-\\hat h_i)^2]-\\\\ \\sum_{i=0}^{K\\times K}\\sum_{j=0}^M I_{ij}^{obj} \\cdot [\\hat C_ilog(C_i)+(1-\\hat C_i)log(1-C_i)]-\\\\ \\sum_{i=0}^{K\\times K}\\sum_{j=0}^M I_{ij}^{noobj} \\cdot [\\hat C_ilog(C_i)+(1-\\hat C_i)log(1-C_i)]-\\\\ \\sum_{i=0}^{K\\times K}\\sum_{j=0}^M \\sum_{c \\in classes} I_{ij}^{obj} \\cdot [\\hat p_i(c)log(p_i(c))+(1-\\hat p_i(c))log(1-(p_i(c))]\\\\$K \\times K$ 是网格数目，$M$ 是每个网格锚框数目，$I_{ij}^{obj}$ 表示 i 号 网格中 j 号锚框是否负责物体，所谓负责物体就是指是否有物体的中心落到这个锚框，如果有物体就落入则值，没物体落入则值为0。$w_i,h_i,x_i,y_i$ 表示网络预测的盒子长宽和中心位置，戴帽子的表示的是真实的长宽和中心位置。$C_i$ 是网络的预测的置信度，带帽是真实置信度。$p_i(c)$ 表示类别为 c 的概率，带帽表示真实概率。 代码实现基础函数yolo_headdef yolo_head(feats, anchors, num_classes, input_shape, calc_loss=False): \"\"\"Convert final layer features to bounding box parameters.\"\"\" num_anchors = len(anchors) # Reshape to batch, height, width, num_anchors, box_params. anchors_tensor = K.reshape(K.constant(anchors), [1, 1, 1, num_anchors, 2]) grid_shape = K.shape(feats)[1:3] # height, width grid_y = K.tile(K.reshape(K.arange(0, stop=grid_shape[0]), [-1, 1, 1, 1]), [1, grid_shape[1], 1, 1]) grid_x = K.tile(K.reshape(K.arange(0, stop=grid_shape[1]), [1, -1, 1, 1]), [grid_shape[0], 1, 1, 1]) grid = K.concatenate([grid_x, grid_y]) grid = K.cast(grid, K.dtype(feats)) feats = K.reshape( feats, [-1, grid_shape[0], grid_shape[1], num_anchors, num_classes + 5]) # Adjust preditions to each spatial grid point and anchor size. box_xy = (K.sigmoid(feats[..., :2]) + grid) / K.cast(grid_shape[::-1], K.dtype(feats)) box_wh = K.exp(feats[..., 2:4]) * anchors_tensor / K.cast(input_shape[::-1], K.dtype(feats)) box_confidence = K.sigmoid(feats[..., 4:5]) box_class_probs = K.sigmoid(feats[..., 5:]) if calc_loss == True: return grid, feats, box_xy, box_wh return box_xy, box_wh, box_confidence, box_class_probs 该函数用于从最终输出的特征图里提取预测框信息 参数 : feats : 特征图，通道数为 5+类别数目 anchors : 特征图中所含锚框，结构为 [[w1,h1],[w2,h2],...] num_classes : 类别数目 input_shape : 原图尺寸信息，(高,宽) calc_loss : 是否用于计算 loss 值 返回 : 如果 calc_loss == True ，则返回 grid, feats, box_xy, box_wh 否则返回 box_xy, box_wh, box_confidence, box_class_probs 其中grid, feats, box_xy, box_wh, box_confidence, box_class_probs 分别是网格坐标信息、原始特征图信息、预测框中心点坐标比例（相对于原图）、预测框大小比例（相对于锚框）、置信度、类别信息。 形状的形状信息为： grid.shape=(特征图高,特征图宽,1,2)feats.shape=(批数,特征图高,特征图宽,锚框数,5+类别数)box_xy.shape=(批数,特征图高,特征图宽,锚框数,2)box_wh.shape=(批数,特征图高,特征图宽,锚框数,2)box_confidence.shape=(批数,特征图高,特征图宽,锚框数,1)box_class_probs.shape=(批数,特征图高,特征图宽,锚框数,类别数) 执行过程 : \"\"\"Convert final layer features to bounding box parameters.\"\"\"num_anchors = len(anchors)# Reshape to batch, height, width, num_anchors, box_params.anchors_tensor = K.reshape(K.constant(anchors), [1, 1, 1, num_anchors, 2]) 获取锚框数目 num_achors 将 anchors 转化为 tf 张量 anchors_tensor，并将形状改变为shape=1, 1, 1, num_anchors, 2 grid_shape = K.shape(feats)[1:3] # height, widthgrid_y = K.tile(K.reshape(K.arange(0, stop=grid_shape[0]), [-1, 1, 1, 1]),[1, grid_shape[1], 1, 1])grid_x = K.tile(K.reshape(K.arange(0, stop=grid_shape[1]), [1, -1, 1, 1]),[grid_shape[0], 1, 1, 1])grid = K.concatenate([grid_x, grid_y])grid = K.cast(grid, K.dtype(feats)) 获取特征图尺寸 grid_shape 通过 K.arange(0, stop=grid_shape[0]) 生成一个长度为特征图高度的向量，元素值是0,1,2,...,grid_shape[0]-1 ; 用a 代指上述向量，利用 K.reshape(a, [-1, 1, 1, 1]) 将上述向量变成 4 维张量，张量 shape=(特征图高,1,1,1) ；用 b 代指上述张量，利用 K.tile(b, [1, grid_shape[1], 1, 1]) 将上述张量，变成 shape=(特征图高,特征图宽,1,1) 的 4 维向量，将结果记为 grid_y grid_y 表示每个格子的纵坐标，比如grid_y[5,9,0,0]==5 意思是：特征图中横坐标是 9 、纵坐标是 5 的像素点的纵坐标是 5，这里特征图的一个像素点被称为一个网格。 相似的手段求 grid_x grid_x = K.tile(K.reshape(K.arange(0, stop=grid_shape[1]), [1, -1, 1, 1]),[grid_shape[0], 1, 1, 1]) ，注意求 grid_x 和求 grid_y 不一样的地方在于 -1 的位置。 grid_y[5,9,0,0]==9 意思是：特征图中横坐标是 9、纵坐标是 5 的网格的横坐标坐标是 5。 通过 K.concatenate 连接 grid_x 和 grid_y 并记为grid，注意K.concatenate 默认是沿着最后一个轴连接。将 grid 转化为浮点型。 grid 的性质是(特征图高,特征图宽,1,2) , 它表示每个格子的横纵坐标，比如：grid[5,9,1]==(9,5) 即横坐标是 9、纵坐标是 5 的格子（像素点）坐标是(9,5) feats = K.reshape( feats, [-1, grid_shape[0], grid_shape[1], num_anchors, num_classes + 5])# Adjust preditions to each spatial grid point and anchor size.box_xy = (K.sigmoid(feats[..., :2]) + grid) / K.cast(grid_shape[::-1], K.dtype(feats))box_wh = K.exp(feats[..., 2:4]) * anchors_tensor / K.cast(input_shape[::-1], K.dtype(feats))box_confidence = K.sigmoid(feats[..., 4:5])box_class_probs = K.sigmoid(feats[..., 5:]) 参数 feats 变化维度为 (批数,特征图高,特征图宽,锚框数,5+类别数) 这样的目的是和标注信息形状一致 计算预测框中心点比例 box_xy，计算方法是：将特征图 0 通道和 1 通道的信息经过K.sigmoid() 压缩至 0 到 1 后，加上网格坐标信息，将上述张量和分别除以网格的高和宽，最终得到一个相对于网格大小的坐标比例信息，因为网格大小由原图大小缩放而来，故可认为相对于网格大小的坐标比例信息，就是相对于原图的坐标比例信息。 同理通过将特征图的 2 通道和 4 通道的数据输送给K.exp()，再乘上锚框张量 anchors_tensor , 除以网格高和宽，得到相对于锚框的大小比例信息 box_wh 将 4 通道数据通过 K.sigmoid() 计算得置信度信息 box_confidence 将剩下的通道的数据通过 K.sigmoid() 计算得到类别概率 if calc_loss == True: return grid, feats, box_xy, box_whreturn box_xy, box_wh, box_confidence, box_class_probs 根据 calc_loss 参数返回相应的变量 box_ioudef box_iou(b1, b2): # Expand dim to apply broadcasting. b1 = K.expand_dims(b1, -2) b1_xy = b1[..., :2] b1_wh = b1[..., 2:4] b1_wh_half = b1_wh/2. b1_mins = b1_xy - b1_wh_half b1_maxes = b1_xy + b1_wh_half # Expand dim to apply broadcasting. b2 = K.expand_dims(b2, 0) b2_xy = b2[..., :2] b2_wh = b2[..., 2:4] b2_wh_half = b2_wh/2. b2_mins = b2_xy - b2_wh_half b2_maxes = b2_xy + b2_wh_half intersect_mins = K.maximum(b1_mins, b2_mins) intersect_maxes = K.minimum(b1_maxes, b2_maxes) intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.) intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1] b1_area = b1_wh[..., 0] * b1_wh[..., 1] b2_area = b2_wh[..., 0] * b2_wh[..., 1] iou = intersect_area / (b1_area + b2_area - intersect_area) return iou 实现 IOU 运算 实现函数yolo_loss关于损失函数都被定义在\\yolo3\\model.py 中： def yolo_loss(args, anchors, num_classes, ignore_thresh=.5, print_loss=False): num_layers = len(anchors)//3 # default setting yolo_outputs = args[:num_layers] y_true = args[num_layers:] anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]] input_shape = K.cast(K.shape(yolo_outputs[0])[1:3] * 32, K.dtype(y_true[0])) grid_shapes = [K.cast(K.shape(yolo_outputs[l])[1:3], K.dtype(y_true[0])) for l in range(num_layers)] loss = 0 m = K.shape(yolo_outputs[0])[0] # batch size, tensor mf = K.cast(m, K.dtype(yolo_outputs[0])) for l in range(num_layers): object_mask = y_true[l][..., 4:5] true_class_probs = y_true[l][..., 5:] grid, raw_pred, pred_xy, pred_wh = yolo_head(yolo_outputs[l], anchors[anchor_mask[l]], num_classes, input_shape, calc_loss=True) pred_box = K.concatenate([pred_xy, pred_wh]) # Darknet raw box to calculate loss. raw_true_xy = y_true[l][..., :2]*grid_shapes[l][::-1] - grid raw_true_wh = K.log(y_true[l][..., 2:4] / anchors[anchor_mask[l]] * input_shape[::-1]) raw_true_wh = K.switch(object_mask, raw_true_wh, K.zeros_like(raw_true_wh)) # avoid log(0)=-inf box_loss_scale = 2 - y_true[l][...,2:3]*y_true[l][...,3:4] # Find ignore mask, iterate over each of batch. ignore_mask = tf.TensorArray(K.dtype(y_true[0]), size=1, dynamic_size=True) object_mask_bool = K.cast(object_mask, 'bool') def loop_body(b, ignore_mask): true_box = tf.boolean_mask(y_true[l][b,...,0:4], object_mask_bool[b,...,0]) iou = box_iou(pred_box[b], true_box) best_iou = K.max(iou, axis=-1) ignore_mask = ignore_mask.write(b, K.cast(best_iou&lt;ignore_thresh, K.dtype(true_box))) return b+1, ignore_mask _, ignore_mask = K.control_flow_ops.while_loop(lambda b,*args: b&lt;m, loop_body, [0, ignore_mask]) ignore_mask = ignore_mask.stack() ignore_mask = K.expand_dims(ignore_mask, -1) # K.binary_crossentropy is helpful to avoid exp overflow. xy_loss = object_mask * box_loss_scale * K.binary_crossentropy(raw_true_xy, raw_pred[...,0:2], from_logits=True) wh_loss = object_mask * box_loss_scale * 0.5 * K.square(raw_true_wh-raw_pred[...,2:4]) confidence_loss = object_mask * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True)+ \\ (1-object_mask) * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True) * ignore_mask class_loss = object_mask * K.binary_crossentropy(true_class_probs, raw_pred[...,5:], from_logits=True) xy_loss = K.sum(xy_loss) / mf wh_loss = K.sum(wh_loss) / mf confidence_loss = K.sum(confidence_loss) / mf class_loss = K.sum(class_loss) / mf loss += xy_loss + wh_loss + confidence_loss + class_loss if print_loss: loss = tf.Print(loss, [loss, xy_loss, wh_loss, confidence_loss, class_loss, K.sum(ignore_mask)], message='loss: ') return loss 参数： args ：包含(yolo_outputs,y_true) ，yolo_outputs 指 YOLO3 模型输出的 y1，y2，y3，这里的输出是含有批维度的，且其第一维度为批维度。y_true 是经过preprocess_true_boxes函数预处理的真实框信息： yolo_outputs 是三元素列表，其中元素分别为[m批13*13特征图张量,m批26*26特征图张量,m批52*52特征图张量]，每张特征图的深度都为图内锚框数*(5+类别数)，所以列表内每个元素的 shape=(批数,特征图宽,特征图高,图内锚框数*(5+类别数)) y_true 是三元素列表，列表内是 np 数组，每个 np 数组对于不同尺寸的特征图，它的形状为 shape=(批数,特征图宽,特征图高,图内锚框数,5+类别数)，每个特征图的尺寸为 13*13、26*26、52*52 关于 yolo_outputs 和 y_true 的形状分析可参考前几篇博文 anchors : 锚框二维数组，结构如[[w1,h1],[w2,h2]..] num_classes ：整型，类别数 ignore_thresh ：浮点型，IOU 小于这个值的将被忽略。 返回： 一维向量，loss值。 执行过程 num_layers = len(anchors)//3 # default settingyolo_outputs = args[:num_layers]y_true = args[num_layers:]anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]]input_shape = K.cast(K.shape(yolo_outputs[0])[1:3] * 32, K.dtype(y_true[0]))grid_shapes = [K.cast(K.shape(yolo_outputs[l])[1:3], K.dtype(y_true[0])) for l in range(num_layers)]loss = 0m = K.shape(yolo_outputs[0])[0] # batch size, tensormf = K.cast(m, K.dtype(yolo_outputs[0])) 获取输出特征图数目 num_layers ，YOLO3 输出 3 张特征图，每张特征图内有 3 个锚框，而tiny-yolo3 则输出两张，故可以根据 len(anchors) 来计算输出特征图数目，以下假设输出特征图数目为 3 通过 num_layers 对参数 args 进行分割，得到 yolo_outputs 和 y_true 定义锚框掩码 anchor_mask , 锚框掩码用于给每个输出特征图分配锚框。 第一个输出特征图的尺寸为 K.shape(yolo_outputs[0])[1:3]，由第一个输出特征图的尺寸*32，可知原图尺寸 input_shape 由每个输出特征图的尺寸 K.shape(yolo_outputs[0])[1:3] 可求每个特征图内的网格信息 grid_shapes 。 求批大小m ，并将批大小m 转化成 tf.Tensor 类型，记为 mf for l in range(num_layers): 之后是对每张输出特征图的操作，以下假设操作第 l 号特征图 object_mask = y_true[l][..., 4:5]true_class_probs = y_true[l][..., 5:]grid, raw_pred, pred_xy, pred_wh = yolo_head(yolo_outputs[l], anchors[anchor_mask[l]], num_classes, input_shape, calc_loss=True)pred_box = K.concatenate([pred_xy, pred_wh]) 从 y_true 里获得物体掩码 object_mask 和类别概率 true_class_probs 为了由 l 号特征图的信息提取得到预测框相关信息，调用 yolo_head(yolo_outputs[l], anchors[anchor_mask[l]], num_classes, input_shape, calc_loss=True)，并将其返回值记为 grid, raw_pred, pred_xy, pred_wh，分别表示：网格坐标、原始特征图信息、预测框的中心点比例信息（相对于原图的比例）、预测框的大小比例信息（相对于锚框的比例） 计算预测框信息 pred_box ，它的值就是合并预测的位置信息和预测的大小信息 pred_box = K.concatenate([pred_xy, pred_wh]) # Darknet raw box to calculate loss.raw_true_xy = y_true[l][..., :2]*grid_shapes[l][::-1] - gridraw_true_wh = K.log(y_true[l][..., 2:4] / anchors[anchor_mask[l]] * input_shape[::-1])raw_true_wh = K.switch(object_mask, raw_true_wh, K.zeros_like(raw_true_wh)) # avoid log(0)=-infbox_loss_scale = 2 - y_true[l][...,2:3]*y_true[l][...,3:4] 对真实盒子信息进行处理，按逆运算，从 l 号特征图的真是信息 y_true[l] 中求出真实框的中心点比例信息 raw_true_xy 和真实框大小比例信息 raw_true_wh 利用 K.switch ，通过 object_mask 对 raw_true_wh 进行修正，raw_true_wh 中含有物体的网格包含真实框的大小比例信息，不包含物体的网格大小为 0 定义修正比例 box_loss_scale 这个比例被赋值为 2-w*h ，最终要乘到坐标和大小的误差项中，意味着 loss 函数对小物体的误差比大物体的误差更敏感。 # Find ignore mask, iterate over each of batch.ignore_mask = tf.TensorArray(K.dtype(y_true[0]), size=1, dynamic_size=True)object_mask_bool = K.cast(object_mask, 'bool')def loop_body(b, ignore_mask): true_box = tf.boolean_mask(y_true[l][b,...,0:4], object_mask_bool[b,...,0]) iou = box_iou(pred_box[b], true_box) best_iou = K.max(iou, axis=-1) ignore_mask = ignore_mask.write(b, K.cast(best_iou&lt;ignore_thresh, K.dtype(true_box))) return b+1, ignore_mask_, ignore_mask = K.control_flow_ops.while_loop(lambda b,*args: b&lt;m, loop_body, [0, ignore_mask])ignore_mask = ignore_mask.stack()ignore_mask = K.expand_dims(ignore_mask, -1) 定义 ignore_mask ，它的形状与 pred_box 只有最后一个维度不同，ignore_mask 的最后一维为 1，通过 tf 的静态图控制语句，动态定义一个掩码张量，用每批预测框张量与 l 层特征图中的每批真实框张量进行 IOU 运算，如果一个预测框存在一个与 IOU 大于参数ignore_thresh 的真是框，就在这个 ignore_mask 中把这个预测框对应位置的元素置值为 1 ，将无效预测框过滤掉。 # K.binary_crossentropy is helpful to avoid exp overflow.xy_loss = object_mask * box_loss_scale * K.binary_crossentropy(raw_true_xy, raw_pred[...,0:2], from_logits=True)wh_loss = object_mask * box_loss_scale * 0.5 * K.square(raw_true_wh-raw_pred[...,2:4])confidence_loss = object_mask * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True)+ \\(1-object_mask) * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True) * ignore_maskclass_loss = object_mask * K.binary_crossentropy(true_class_probs, raw_pred[...,5:], from_logits=True) 按照损失函数计算 xy_loss 、wh_loss、confidence_loss 、class_loss xy_loss = K.sum(xy_loss) / mfwh_loss = K.sum(wh_loss) / mfconfidence_loss = K.sum(confidence_loss) / mfclass_loss = K.sum(class_loss) / mfloss += xy_loss + wh_loss + confidence_loss + class_loss 将 l 号特征图的各 loss 项求和并除批大小，最终化为标量，随后将各标量加和到 loss return loss 处理好所有特征图后，返回 loss ，loss.shape=(1,) 训练模型基础函数get_classesdef get_classes(classes_path): '''loads the classes''' with open(classes_path) as f: class_names = f.readlines() class_names = [c.strip() for c in class_names] return class_names 从 classes_path 中按行读取类别，构建类别列表并返回 get_anchorsdef get_anchors(anchors_path): '''loads the anchors from a file''' with open(anchors_path) as f: anchors = f.readline() anchors = [float(x) for x in anchors.split(',')] return np.array(anchors).reshape(-1, 2) 从 anchors_path 按行读取锚框信息，构建锚框列表并返回 create_model()def create_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2, weights_path='model_data/yolo_weights.h5'): '''create the training model''' K.clear_session() # get a new session image_input = Input(shape=(None, None, 3)) h, w = input_shape num_anchors = len(anchors) y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\ num_anchors//3, num_classes+5)) for l in range(3)] model_body = yolo_body(image_input, num_anchors//3, num_classes) print('Create YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes)) if load_pretrained: model_body.load_weights(weights_path, by_name=True, skip_mismatch=True) print('Load weights {}.'.format(weights_path)) if freeze_body in [1, 2]: # Freeze darknet53 body or freeze all but 3 output layers. num = (185, len(model_body.layers)-3)[freeze_body-1] for i in range(num): model_body.layers[i].trainable = False print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers))) model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss', arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5})( [*model_body.output, *y_true]) model = Model([model_body.input, *y_true], model_loss) return model 参数： input_shape : 二维列表或张量，输入图片的尺寸，高在前，宽在后 anchors : 锚框信息列表，结构类似 [[w1,h1]...] num_classes ：整型，类别数目 load_pretrained ：是否加载预训练的权重，默认是 True freeze_body：设置冻结那些层 weights_path：预训练权重的存储路径 分段讲解： K.clear_session() # get a new sessionimage_input = Input(shape=(None, None, 3))h, w = input_shapenum_anchors = len(anchors) 创建 Input 类型张量作为静态图的输入结点，从 input_shape 中分离高 h 和宽 w，获取锚框数目 num_anchors y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\ num_anchors//3, num_classes+5)) for l in range(3)] 构建 y_true ，y_true 是一个拥有三个 Input 张量的列表，三个 Input 张量的尺寸分别为 原图长宽/32,原图窗口/16,原图长宽/8，每个 Input 张量的通道数都是 类别数+5 model_body = yolo_body(image_input, num_anchors//3, num_classes)print('Create YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes)) 构建 yolo 网络，记为model_body，通过 yolo_body yolo_body 是一个定义在 model.py 的函数，里面定义了 输入结点里的数据在静态图中的流转过程。 if load_pretrained: model_body.load_weights(weights_path, by_name=True, skip_mismatch=True) print('Load weights {}.'.format(weights_path)) if freeze_body in [1, 2]: # Freeze darknet53 body or freeze all but 3 output layers. num = (185, len(model_body.layers)-3)[freeze_body-1] for i in range(num): model_body.layers[i].trainable = False print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers))) 根据参数 load_pretrained 判断是否需要加载预训练权重 如果需要加载预训练权重，则从预训练权重路径加载权重，并忽略不匹配的层 计算总共需要冻结的层 num ，load_pretrained=1 则冻结 darknet53，如果load_pretrained=2 就除了倒数三层（三个用于输出的层）其余层都冻结。 把 0 到 num 层全冻结。 model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss', arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5})( [*model_body.output, *y_true])model = Model([model_body.input, *y_true], model_loss)return model 使用 keras.layers.Lambda 自定义一个层，这个层使用的是 yolo_loss 函数，层的输出形状是 (1,)，名称为 yolo_loss，输入的参数是 {'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5} 使用 Lambda 可以将一个函数转化为层对象，使之可以被添加进模型之中，要求函数的第一个参数是输的入张量，其余参数可以通过 arguments 以字典的形式传入 将 [*model_body.output, *y_true] 作为输入的张量，输入到 Lambda 定义的层的，将其返回的张量定义为 model_loss ，该过程另静态图中衔接了新的结构，训练时从输入结点输入的数据将会先通过 model_body 静态图，再通过 Lambda 定义的静态图，最后输送到输出结点 使用 keras.models.Model 将 [model_body.input, *y_true] 作为输入结点，将 model_loss 作为输出结点，构建静态图，并返回模型。 create_model() 创建的并非是 YOLO3 网络本身，而是 YOLO3 网络，加上 loss 函数层，在定义 loss 函数时，只需要将最后一层的输出作为最终loss值即可，也就是说，loss 函数的运算部分作为模型的最后层被 create_model() 所创建。 实现函数def _main(): annotation_path = 'train.txt' log_dir = 'logs/000/' classes_path = 'model_data/voc_classes.txt' anchors_path = 'model_data/yolo_anchors.txt' class_names = get_classes(classes_path) num_classes = len(class_names) anchors = get_anchors(anchors_path) input_shape = (416,416) # multiple of 32, hw is_tiny_version = len(anchors)==6 # default setting if is_tiny_version: model = create_tiny_model(input_shape, anchors, num_classes, freeze_body=2, weights_path='model_data/tiny_yolo_weights.h5') else: model = create_model(input_shape, anchors, num_classes, freeze_body=2, weights_path='model_data/yolo_weights.h5') # make sure you know what you freeze logging = TensorBoard(log_dir=log_dir) checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5', monitor='val_loss', save_weights_only=True, save_best_only=True, period=3) reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1) early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1) val_split = 0.1 with open(annotation_path) as f: lines = f.readlines() np.random.seed(10101) np.random.shuffle(lines) np.random.seed(None) num_val = int(len(lines)*val_split) num_train = len(lines) - num_val # Train with frozen layers first, to get a stable loss. # Adjust num epochs to your dataset. This step is enough to obtain a not bad model. if True: model.compile(optimizer=Adam(lr=1e-3), loss={ # use custom yolo_loss Lambda layer. 'yolo_loss': lambda y_true, y_pred: y_pred}) batch_size = 32 print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size)) model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes), steps_per_epoch=max(1, num_train//batch_size), validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes), validation_steps=max(1, num_val//batch_size), epochs=50, initial_epoch=0, callbacks=[logging, checkpoint]) model.save_weights(log_dir + 'trained_weights_stage_1.h5') # Unfreeze and continue training, to fine-tune. # Train longer if the result is not good. if True: for i in range(len(model.layers)): model.layers[i].trainable = True model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change print('Unfreeze all of the layers.') batch_size = 32 # note that more GPU memory is required after unfreezing the body print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size)) model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes), steps_per_epoch=max(1, num_train//batch_size), validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes), validation_steps=max(1, num_val//batch_size), epochs=100, initial_epoch=50, callbacks=[logging, checkpoint, reduce_lr, early_stopping]) model.save_weights(log_dir + 'trained_weights_final.h5') # Further training if needed. 分段讲解： annotation_path = 'train.txt'log_dir = 'logs/000/'classes_path = 'model_data/voc_classes.txt'anchors_path = 'model_data/yolo_anchors.txt' 配置相关文件路径，annotation_path 是由 voc_annotation.py 生成的 YOLO3 格式的批注文件的路径, log_dir 用于保存 TensorBoard 和 checkpoint;classes_path 是 YOLO3 类别文件路径; anchors_path 是 YOLO3 锚框(预设框)文件路径 class_names = get_classes(classes_path)num_classes = len(class_names)anchors = get_anchors(anchors_path)input_shape = (416,416) # multiple of 32, hwis_tiny_version = len(anchors)==6 # default setting 定义相关变量，class_names 是由 get_classes(classes_path) 生成的类别名数组; num_classes 是类别数据中的类别的数目; anchors 是由 get_anchors(anchors_path) 生成的锚框坐标大小信息数组; input_shape 定义输入图像的尺寸，高在前宽在后。is_tiny_version 通过判断锚框数目断定是否是 tiny yolo。 if is_tiny_version: model = create_tiny_model(input_shape, anchors, num_classes, freeze_body=2, weights_path='model_data/tiny_yolo_weights.h5')else: model = create_model(input_shape, anchors, num_classes, freeze_body=2, weights_path='model_data/yolo_weights.h5') # make sure you know what you freeze 根据 is_tiny_version 变量判断使用的网络版本,不同版本通过不同函数创建网络,如 yolo3 网络通过 create_model() 创建. 使用 create_model() 创建模型的时候，通过参数设计冻结两层，并设置好预训练权重路径 logging = TensorBoard(log_dir=log_dir)checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5', monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1) 设置 TensorBoard 回调，用于保存训练过程中的信息，进行可视化，实例名记为 logging 设置 ModelCheckpoint 回调函数，用于保存训练过程的权重，参数解读：保存权重的路径为 log_dir ，文件名格式为ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5 ，监视的目标是在测试集的loss值 val_loss ，只保存权重不保存模型结构，只保存结果好与已有结果的权重，每 3 轮保存一次。记实例名为 checkpoint 设置 ReduceLROnPlateau 回调，当 loss 不再下降的时候，按照一定因数下调学习率，继续训练。根据参数解读：监视的 loss 为训练集上的 loss 值 val_loss ,每次调整学习率的因数为 0.1，根据公式 new_lr = lr * factor 可知，每次将学习率缩小十倍，如果 val_loss 连续 3 轮没有下降，则降低学习率，日志模式为 1。记实例名为 reduce_lr。 设置 EarlyStopping 回调，当 loss 值不再下降时，提前停止训练，参数解读：监视目标为 val_loss ，最小下降值为 0，如果连续 10 论没有下降则早停，日志模型为 1。记实例名为 early_stopping val_split = 0.1with open(annotation_path) as f: lines = f.readlines()np.random.seed(10101)np.random.shuffle(lines)np.random.seed(None)num_val = int(len(lines)*val_split)num_train = len(lines) - num_val 给定测试集划分比例为 0.1 ，总载入样本的 val_split 部分将会作为测试集，不参与训练。 将 annotation_path 中的信息读入到 lines 列表中 对 lines 按照 val_split 进行划分，将测试数目记为 num_val, 训练集数目记为 num_train # Train with frozen layers first, to get a stable loss.# Adjust num epochs to your dataset. This step is enough to obtain a not bad model.if True: model.compile(optimizer=Adam(lr=1e-3), loss={ # use custom yolo_loss Lambda layer. 'yolo_loss': lambda y_true, y_pred: y_pred}) batch_size = 32 print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size)) model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes), steps_per_epoch=max(1, num_train//batch_size), validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes), validation_steps=max(1, num_val//batch_size), epochs=50, initial_epoch=0, callbacks=[logging, checkpoint]) model.save_weights(log_dir + 'trained_weights_stage_1.h5') 进行第一阶段训练,该阶段会冻结主干网络的权重(已经已经预加载了一些),通过调整部分权重使模型快速的 loss 快速下降到一个可接受的值。 编译模型，使用 Adam 优化器，名为 yolo_loss 的输出经过函数 lambda y_true, y_pred: y_pred 得到 loss 值 这里的 loss 函数把模型最后一层的输出作为 loss 值，原因是 create_model() 把 loss 函数的运算部分定义在了模型的最后一层 批大小设置为 32 训练数据，通过 data_generator_wrapper() 生成器生成训练数据，用 data_generator_wrapper 生成验证数据，使用 logging 和 checkpoint 作为回调对象 将训练好的权重保存起来 if True: for i in range(len(model.layers)): model.layers[i].trainable = True model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change print('Unfreeze all of the layers.') batch_size = 32 # note that more GPU memory is required after unfreezing the body print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size)) model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes), steps_per_epoch=max(1, num_train//batch_size), validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes), validation_steps=max(1, num_val//batch_size), epochs=100, initial_epoch=50, callbacks=[logging, checkpoint, reduce_lr, early_stopping]) model.save_weights(log_dir + 'trained_weights_final.h5') 第二阶段训练，这个阶段将会解冻所有层，在第一阶段训练的基础上进行微调。 用 for 解冻所有层 编译模型，与第一阶段相同 批大小为 32 训练模型，数据载入与第一阶段相同，使用 logging,checkpoint,reduce_lr,early_stopping 作为回调 保存最终模型","link":"/2020/04/04/YOLO3%20%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0%20Keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"YOLO3 数据处理与数据加载 Keras源码分析","text":"YOLO3 Keras 源码：https://github.com/qqwweee/keras-yolo3 前言本文从主要是从源码层面对 YOLO3 的数据处理相关内容进行分析与讲解。通常，一个功能的实现需要多个函数配合，所以我将每个功能的实现函数分为两部分，基础函数 和 实现函数 ： 基础函数：被实现函数所依赖，是实现函数的一部分 实现函数：通过调用基础函数实现功能 源码内容比较多，通过目录索引看感兴趣的地方即可。 数据处理数据集转换YOLO3 Keras 源码所需要的数据集应该在一个.txt ( 文本文件 )内，文件中的一行代表一个张图片和它的标签，其中每行的格式为： image_file_path box1 box2 ... boxN box* 是该图片的标签，即真实框，不同box之间用空格隔开，其中每个box的格式为 x_min,y_min,x_max,y_max,class_id 所以总的来说，训练用的数据集应该大体是这个样子的： path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3path/to/img2.jpg 120,300,250,600,2... 对于 VOC 数据集，Keras 源码的作者给出了脚本 voc_annotation.py 可以将 VOC 数据集转化成上述格式。 源码分析import xml.etree.ElementTree as ETfrom os import getcwdsets=[('2007', 'train'), ('2007', 'val'), ('2007', 'test')]classes = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]def convert_annotation(year, image_id, list_file): in_file = open('VOCdevkit/VOC%s/Annotations/%s.xml'%(year, image_id)) tree=ET.parse(in_file) root = tree.getroot() for obj in root.iter('object'): difficult = obj.find('difficult').text cls = obj.find('name').text if cls not in classes or int(difficult)==1: continue cls_id = classes.index(cls) xmlbox = obj.find('bndbox') b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text), int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text)) list_file.write(\" \" + \",\".join([str(a) for a in b]) + ',' + str(cls_id))wd = getcwd()for year, image_set in sets: image_ids = open('VOCdevkit/VOC%s/ImageSets/Main/%s.txt'%(year, image_set)).read().strip().split() list_file = open('%s_%s.txt'%(year, image_set), 'w') for image_id in image_ids: list_file.write('%s/VOCdevkit/VOC%s/JPEGImages/%s.jpg'%(wd, year, image_id)) convert_annotation(year, image_id, list_file) list_file.write('\\n') list_file.close() 基础函数：convert_annotation(year, image_id, list_file)参数： year：整型，年代 , VOC2007 的 2007 image_id ：图像序号 list_file ： 文件对象 功能： 通过 year 和 image_id找到 VOC 数据集里的 xml 文件，通过 ElementTree 找到这个 XML 里的每一个 object 标签，获取该标签内的 xmin,ymin,xmax,ymax，和列表序号class_id ，把它们写入到 list_file 所打开的文件对象中。 总之，这个函数是在将 .xml 中的标注信息转化为 YOLO3 所需要标注格式，并写入文件。 实现过程脚本主内容for year, image_set in sets: image_ids = open('VOCdevkit/VOC%s/ImageSets/Main/%s.txt'%(year, image_set)).read().strip().split() list_file = open('%s_%s.txt'%(year, image_set), 'w') for image_id in image_ids: list_file.write('%s/VOCdevkit/VOC%s/JPEGImages/%s.jpg'%(wd, year, image_id)) convert_annotation(year, image_id, list_file) list_file.write('\\n') list_file.close() year, image_set 是从 sets 提取而来，这个 for 循环的目的是操作 sets 中对应的每一个文件。 即打开train.txt 、val.txt 、test.txt ，它们里面内容长这个样： 01101001000.... 这其中每一行都是图片的序号。 如打开train.txt ，脚本通过 .read().strip().split() ，把 train.txt 中一行一行的图片序号变成一个列表 image_ids ，创建一个文件对象 list_file 随后对列表内每一个元素进行操作： 根据文件路径和图片序号的映射关系，把图片路劲写入到 list_file 文件对象对应的文件中。 通过 convert_annotation 把图片的标注信息（已转化为 YOLO3 所需格式），紧接着路径信息写在后面 写入换行符 所以元素操作完事，关闭文件对象。 通过上述流程对 train.txt 、val.txt 、test.txt 都操作完，就把 VOC 数据集转化为 YOLO3 所需要的格式了。 使用 K-means 生成锚框YOLO2 开始，锚框的生成使用 K-means 算法，这种倚靠训练数据自动生成的锚框，比人为设定的锚框更加符合特定场景的业务需求，所以会提高模型的准确度。 介绍 K-means 之前，需要先了解它是用于什么学习任务的，K-means 是解决聚类问题的一个算法。 理论基础聚类和簇 聚类任务是一种无监督学习任务，聚类试图将样本划分为若干个不相交的子集，每个子集成为簇，通过这样的划分，每个簇对应一个潜在类别，具体是什么类别，对于聚类算法而言是未知，都是在聚类结束后，人为的判断一个簇对应的什么类别。聚类常用于寻找数据内部的分布结构，或用于对其他分类学习任务进行粗略的预先处理。比如自从 YOLO2 后，聚类算法就被用于预先为图像划定锚框。 距离计算 聚类任务说到底是让”簇内相似度高“且”簇间相似度低“，那么就需要一个衡量相似度的指标，两个样本间的距离可以反映样本之间的相似度，所以定义 $dist(\\cdot,\\cdot)$ 为两个样本的距离。距离的计算有诸多算法，比如“闵可夫斯基距离” dist_{mk}(x_i,x_j)=(\\sum_{u=1}^n|x_{i,u}-x_{j,x}|^p)^{\\frac{1}{p}}在 YOLO2 和 YOLO3 中，距离计算是借助两个 box 的 IOU： dist(b_i,b_j)=1-IOU(b_i,b_j)K-means 算法 K-means 的思想是，对所划分的所有簇 $C={C_1,C_2,\\dots,C_k}$ 使 E=\\sum_{i=1}^k\\sum_{x\\in C_i}dist(x,\\mu_i)最小，其中 $\\mui=\\frac{1}{|C_i|}\\sum{x\\in C_i}x$ ，即 $\\mu_i$ 是 $C_i$ 的均指向量。 为了达到这个目的，K-means 采用贪心策略，最开始随机指定 K 个向量作为 $(\\mu_1,\\dots,\\mu_k)$ 对于每一个 $\\mu_i$ ，计算每一个向量 $x$ 与 $\\mu_i$ 的距离，即 $dist(x,\\mu_i)$ 将距离值最小的向量，归到 $C_i$ 然后更新 $\\mu_i$ ，重复这个过程，直到 $(\\mu_1,\\dots,\\mu_k)$ 都不再更新。 基础函数YOLO_Kmeans.__init__YOLO_Kmeans 是一个类，它的初始化函数如下： def __init__(self, cluster_number, filename): self.cluster_number = cluster_number self.filename = filename 其作用仅是将参数保留到类内 YOLO_Kmeans.txt2boxesdef txt2boxes(self): f = open(self.filename, 'r') dataSet = [] for line in f: infos = line.split(\" \") length = len(infos) for i in range(1, length): width = int(infos[i].split(\",\")[2]) - \\ int(infos[i].split(\",\")[0]) height = int(infos[i].split(\",\")[3]) - \\ int(infos[i].split(\",\")[1]) dataSet.append([width, height]) result = np.array(dataSet) f.close() return result 该方法的作用是将 YOLO 标注文件内的 box 信息转化为列表 YOLO 标注文件格式如下： path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3path/to/img2.jpg 120,300,250,600,2... f 是一个文件对象，文件对象可迭代，每次读取文件的一行，最外层的 for 是指对文件内的每一行操作。 对文件内的每一行，以空格为分隔符，将其划分为列表 infos ： 对 infos[1] 到 infors[length-1] 中的每一个元素 infos[i] 而言，每一个 infos[i] 代表一组 box 信息。infos[i].split(&quot;,&quot;)[0] 、infos[i].split(&quot;,&quot;)[1] 、infos[i].split(&quot;,&quot;)[2] 、infos[i].split(&quot;,&quot;)[3] 、infos[i].split(&quot;,&quot;)[4]分别对应 box 信息里的 x_min,y_min,x_max,y_max,class_id ，由这些信息就可以计算出 box 的宽和高，记为[width, height] ，将每组 box 的 [width, height] 追加到 dataSet 列表内，将 dataSet 转化为 np.array 并返回。 YOLO_Kmeans.ioudef iou(self, boxes, clusters): # 1 box -&gt; k clusters n = boxes.shape[0] k = self.cluster_number box_area = boxes[:, 0] * boxes[:, 1] box_area = box_area.repeat(k) box_area = np.reshape(box_area, (n, k)) cluster_area = clusters[:, 0] * clusters[:, 1] cluster_area = np.tile(cluster_area, [1, n]) cluster_area = np.reshape(cluster_area, (n, k)) box_w_matrix = np.reshape(boxes[:, 0].repeat(k), (n, k)) cluster_w_matrix = np.reshape(np.tile(clusters[:, 0], (1, n)), (n, k)) min_w_matrix = np.minimum(cluster_w_matrix, box_w_matrix) box_h_matrix = np.reshape(boxes[:, 1].repeat(k), (n, k)) cluster_h_matrix = np.reshape(np.tile(clusters[:, 1], (1, n)), (n, k)) min_h_matrix = np.minimum(cluster_h_matrix, box_h_matrix) inter_area = np.multiply(min_w_matrix, min_h_matrix) result = inter_area / (box_area + cluster_area - inter_area) return result 参数： boxes ： 盒子信息，结构为[[w1,h1],[w2,h2]...] clusters：簇点信息，内含 k 个 [w,h] 结构为 [[w1,h1],...,[wk,hk]] 执行过程： 求每个盒子的面积，重复 k 次，得到 n*k 矩阵 box_area , n 是总盒子数，k 是簇数 求每个簇点的面积，所有簇点面积集合记为 cluster_area , 一开始，cluster_area 是 shape=(1,k) 的 np 数组，经过 np.tile 变成 shape=(1,k*n)，np.reshape 成 shape=(n,k) 用相似的维度变化，求盒子的 w 矩阵 box_w_matrix，与簇点的 w 矩阵 cluster_w_matrix ，用 np.minimum 取宽最小矩阵 min_w_matrix 同理求高最小矩阵 min_h_matrix 盒子与簇在假设左上角对齐的情况下，最小的宽*最小的高就是他们相交的区域的面积，相交面积 inter_area = np.multiply(min_w_matrix, min_h_matrix) 最终返回 IOU 值 IOU = 相交面积/相并面积 = 相交面积/( 盒子面积 + 簇点面积 - 相交面积 )，最红返回的 result 的形状 shape=(n,k) YOLO_Kmeans.kmeansdef kmeans(self, boxes, k, dist=np.median): box_number = boxes.shape[0] distances = np.empty((box_number, k)) last_nearest = np.zeros((box_number,)) np.random.seed() clusters = boxes[np.random.choice( box_number, k, replace=False)] # init k clusters while True: distances = 1 - self.iou(boxes, clusters) current_nearest = np.argmin(distances, axis=1) if (last_nearest == current_nearest).all(): break # clusters won't change for cluster in range(k): clusters[cluster] = dist( # update clusters boxes[current_nearest == cluster], axis=0) last_nearest = current_nearest return clusters 参数 : boxes : 前面通过 txt2boxes 处理后得到的 np 数组，结构为[[w1,h1],[w2,h2]...] k : 整型，簇数 dist : 均值向量生成策略，这里用的中位数（那应该叫中位数向量hhhh） 执行过程： 得到 boxes 中的盒子数 box_number，用 np.empty 创建一个 shape=(box_number, k) 的空 np 数组 distances，用储存 boxes 中的每个盒子和每个簇的距离；用 np.zeros 创建一个长度为 box_number 的一维全零 np 数组，用于存储每个盒子距离最近的簇的索引值。 通过 clusters = boxes[np.random.choice(box_number, k, replace=False)] 从 boxes 中随机进行不放回抽样，选出 k 个 盒子信息作为初始的簇点。 这里采用的是高级索引，boxes[[1,2,3]]==[boxes[1],boxes[2],boxes[3]] ，numpy.random.choice(a, size=None, replace=True, p=None) 它的参数从左到右依次表示候选列表、抽烟个数、是否放回、列表中各元素被抽中的概率。 进行循环直到 break 才终止： 计算boxes 中的每个盒子和每个簇的距离，结果存放入 distances， self.iou(boxes, clusters) 后面讲解。 通过 np.argmin(distances, axis=1) 返回每个盒子距离最近的簇的索引值，结果存入current_nearest；np.argmin(a, axis=None, out=None) 可以沿 axis轴，在数组 a 中寻找最小值，并返回该值在 axis 轴内的索引。 如果上一轮循环中每个盒子距离最近的簇的索引 last_nearest 和这一轮的 current_nearest 中每一个元素都相同，那么说明算法已经收敛，跳出循环。 其中 last_nearest == current_nearest 由于双等号两边都是同型 np 数组，所以这个表达式返回的也是同型 np 数组，数组内元素类型是布尔型，表示对应元素是否相同，np.np.array.all() 表示若数组内元素都为 True 则返回 True，否则返回 False 如果存在不同元素，就对 K 个簇点进行遍历， 让每个簇点等于该簇内向量的中位数。 通过boxes[current_nearest == cluster] 找出所有”相对于其他簇点，与第cluster 个簇距离最近”的盒子，得到一个列表，列表结构为[[w1,h1],[w2,h2]...] 通过 dist 即 np.median 对上述列表，沿第一个轴求中位数，最终得到结构为[w,h]的一维二元列表。 更新结束，把当前状态 current_nearest 赋值给 last_nearest 开始下一轮循环。 循环结束，将簇点列表 clusters 返回，在 YOLO3 中，各簇点就是预设的锚框信息。 YOLO_Kmeans.txt2clustersdef txt2clusters(self): = self.txt2boxes() result = self.kmeans(all_boxes, k=self.cluster_number) result = result[np.lexsort(result.T[0, None])] self.result2txt(result) print(\"K anchors:\\n {}\".format(result)) print(\"Accuracy: {:.2f}%\".format( self.avg_iou(all_boxes, result) * 100)) txt2clusters 是脚本的执行函数，求kmeans就是从这个函数入手，这个函数主要过程就是： txt2boxes 将文本信息划分并提取成列表信息 all_boxes , all_boxes 内各个盒子的信息，结构为 [[w1,h1],[w2,h2]...] 使用 kmeans 求盒子列表的聚类，返回的是 k 个簇点，结构也为 [[w1,h1],[w2,h2]...] j结果保存到 result 通过 result[np.lexsort(result.T[0, None])] 对 result 进行排序，排序规则是宽度小的在前面 result.T 会对 result 转置，转置后结构为 [[w1,...,wk],[h1...hk]] result.T[0,None] 是只取宽度数据，并且扩充一维，变成[[w1,...,wk]] lexsort(keys, axis=None) ：返回一个整数数组（返回数组的元素是索引值）。keys 是一个序列的序列，要求内部序列形状相同，默认情况外部序列 keys 中的最后一个序列元素将作为主排序序列，倒数第二个序列元素作为第二排序序列….当主排序序列中元素大小相同时，就依照第二排序系列进行排序。 np.lexsort(result.T[0, None]) 输出结果是 w1,...,wk 依照升序排序后的顺序输出原索引值，是有 K 个元素的一维数组 使用 result2txt 将结果输出为文本文件 使用 avg_iou 评价聚类的结果 YOLO_Kmeans.result2txtdef result2txt(self, data): f = open(\"yolo_anchors.txt\", 'w') row = np.shape(data)[0] for i in range(row): if i == 0: x_y = \"%d,%d\" % (data[i][0], data[i][1]) else: x_y = \", %d,%d\" % (data[i][0], data[i][1]) f.write(x_y) f.close() 将每一个簇点信息，写入 yolo_anchors.txt 文件，文件格式是： w1,h1 w2,h2 ... wk,hk YOLO_Kmeans.avg_ioudef avg_iou(self, boxes, clusters): accuracy = np.mean([np.max(self.iou(boxes, clusters), axis=1)]) return accuracy 用于评价聚类效果，这里的准确率是 n 个盒子的平均最大交并比。对于每个盒子来说，与盒子 IOU 最大的簇，一定是这个盒子所属的簇，所以 acc 可以理解为是 n 个盒子和它所属的簇的 IOU 的均值。acc 越大，说明簇分布的越好。 具体算法是 Accuracy=\\frac{\\sum_{i=1}^n IOU_{max}(Box_i)}{n}其中， IOU_{max}(Box_i)=\\max_{i}\\{ IOU(Box_i,C_j) \\}即 n 个盒子和 k 个簇进行 IOU 运算，对于每个盒子选择 IOU 最大的值作为代表，最终求 n 个 IOU 的均值作为准确率。 实现函数在 \\kmeans.py 文件中： if __name__ == \"__main__\": cluster_number = 9 filename = \"2007_train.txt\" kmeans = YOLO_Kmeans(cluster_number, filename) kmeans.txt2clusters() 整体来说，这个脚本干了两件事，一个是求符合 YOLO 要求的 Kmeans，一个是把这个结果保存成文本。 读入数据YOLO3 需要将训练数据载入后进行一些处理才能用于训练，这部分代码主要位于 train.py 文件。 基础函数get_random_data在yolo3\\utils.py def get_random_data(annotation_line, input_shape, random=True, max_boxes=20, jitter=.3, hue=.1, sat=1.5, val=1.5, proc_img=True): '''为实现实时数据增强的随机预处理''' line = annotation_line.split() # 打开图片 image = Image.open(line[0]) iw, ih = image.size h, w = input_shape box = np.array([np.array(list(map(int,box.split(',')))) for box in line[1:]]) #box = [[x_min,y_min,x_max,y_max,class_id],..] if not random: # 调整图像尺寸,使其与input_shape一致 scale = min(w/iw, h/ih) nw = int(iw*scale) nh = int(ih*scale) dx = (w-nw)//2 dy = (h-nh)//2 image_data=0 if proc_img: image = image.resize((nw,nh), Image.BICUBIC) new_image = Image.new('RGB', (w,h), (128,128,128)) new_image.paste(image, (dx, dy)) image_data = np.array(new_image)/255. # 修正 boxes box_data = np.zeros((max_boxes,5)) if len(box)&gt;0: np.random.shuffle(box) if len(box)&gt;max_boxes: box = box[:max_boxes] box[:, [0,2]] = box[:, [0,2]]*scale + dx box[:, [1,3]] = box[:, [1,3]]*scale + dy box_data[:len(box)] = box return image_data, box_data # 随机调整图像长宽比，长宽比改变范围由 jitter 参数控制 new_ar = w/h * rand(1-jitter,1+jitter)/rand(1-jitter,1+jitter) #随机缩放图像，缩放范围为（0.25，2） scale = rand(.25, 2) if new_ar &lt; 1: nh = int(scale*h) nw = int(nh*new_ar) else: nw = int(scale*w) nh = int(nw/new_ar) image = image.resize((nw,nh), Image.BICUBIC) # 图像随机平移 # 调整图像尺寸,使其与input_shape一致，多的裁掉，少的用灰色填空 dx = int(rand(0, w-nw)) dy = int(rand(0, h-nh)) new_image = Image.new('RGB', (w,h), (128,128,128)) new_image.paste(image, (dx, dy)) image = new_image # 随机左右翻转，翻转概率 0.5 flip = rand()&lt;.5 if flip: image = image.transpose(Image.FLIP_LEFT_RIGHT) # 对图像的色调、饱和度、明度进行随机调整 hue = rand(-hue, hue) sat = rand(1, sat) if rand()&lt;.5 else 1/rand(1, sat) val = rand(1, val) if rand()&lt;.5 else 1/rand(1, val) # HUE 色调; Saturation 饱和度; Value 明度 x = rgb_to_hsv(np.array(image)/255.) x[..., 0] += hue x[..., 0][x[..., 0]&gt;1] -= 1 x[..., 0][x[..., 0]&lt;0] += 1 x[..., 1] *= sat x[..., 2] *= val x[x&gt;1] = 1 x[x&lt;0] = 0 image_data = hsv_to_rgb(x) # numpy array, 0 to 1 # 修正盒子信息 box_data = np.zeros((max_boxes,5)) if len(box)&gt;0: np.random.shuffle(box) # box 缩放与位移 box[:, [0,2]] = box[:, [0,2]]*nw/iw + dx box[:, [1,3]] = box[:, [1,3]]*nh/ih + dy # box 反转 if flip: box[:, [0,2]] = w - box[:, [2,0]] # box 截掉超出边界的 box[:, 0:2][box[:, 0:2]&lt;0] = 0 box[:, 2][box[:, 2]&gt;w] = w box[:, 3][box[:, 3]&gt;h] = h # box 丢弃过小的 box_w = box[:, 2] - box[:, 0] box_h = box[:, 3] - box[:, 1] box = box[np.logical_and(box_w&gt;1, box_h&gt;1)] # discard invalid box if len(box)&gt;max_boxes: box = box[:max_boxes] box_data[:len(box)] = box return image_data, box_data get_random_data 函数用于从 YOLO3 的标注文件生成训练所需的图像信息，同时还提供了对图像的随机数据增强。具体实现看注释吧。 参数： annotation_line：标注文件中某一行的文本信息，通过这一行文本来获取图片路径和盒子信息。 input_shape：指网络的输入尺寸，最终图像会被调整到这个尺寸 random=True：是否开启随机数据增强，默认开启，如果不开启就是单纯的缩放个尺寸 max_boxes=20：最大盒子数，默认20 jitter=.3：开启随机数据增强时，这个参数影响长宽比变化范围 hue=.1：开启随机数据增强时，这个参数影响色调变化范围 sat=1.5：开启随机数据增强时，这个参数影响饱和度变化范围 val=1.5：开启随机数据增强时，这个参数影响明度变化范围 proc_img=True：没整明白存在的意义，True 就完事了 返回值： 返回一个元组，(image_data, box_data) image_data 形如 (h,w,3)，值域 0 到 1 box_data 二维数组，结构类似 [[x_min,y_min,x_max,y_max,class_id],..]，它的形状为 shape=(max_boxes,5)，5 是指相对于输入尺寸的绝对坐标 x_min, y_min, x_max, y_max, 和类别号 class_id preprocess_true_boxes在yolo3\\model.py def preprocess_true_boxes(true_boxes, input_shape, anchors, num_classes): assert (true_boxes[..., 4]&lt;num_classes).all(), 'class id must be less than num_classes' num_layers = len(anchors)//3 # default setting anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]] true_boxes = np.array(true_boxes, dtype='float32') input_shape = np.array(input_shape, dtype='int32') boxes_xy = (true_boxes[..., 0:2] + true_boxes[..., 2:4]) // 2 boxes_wh = true_boxes[..., 2:4] - true_boxes[..., 0:2] true_boxes[..., 0:2] = boxes_xy/input_shape[::-1] true_boxes[..., 2:4] = boxes_wh/input_shape[::-1] m = true_boxes.shape[0] grid_shapes = [input_shape//{0:32, 1:16, 2:8}[l] for l in range(num_layers)] y_true = [np.zeros((m,grid_shapes[l][0],grid_shapes[l][1],len(anchor_mask[l]),5+num_classes), dtype='float32') for l in range(num_layers)] # Expand dim to apply broadcasting. anchors = np.expand_dims(anchors, 0) anchor_maxes = anchors / 2. anchor_mins = -anchor_maxes valid_mask = boxes_wh[..., 0]&gt;0 for b in range(m): # Discard zero rows. wh = boxes_wh[b, valid_mask[b]] if len(wh)==0: continue # Expand dim to apply broadcasting. wh = np.expand_dims(wh, -2) box_maxes = wh / 2. box_mins = -box_maxes intersect_mins = np.maximum(box_mins, anchor_mins) intersect_maxes = np.minimum(box_maxes, anchor_maxes) intersect_wh = np.maximum(intersect_maxes - intersect_mins, 0.) intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1] box_area = wh[..., 0] * wh[..., 1] anchor_area = anchors[..., 0] * anchors[..., 1] iou = intersect_area / (box_area + anchor_area - intersect_area) # Find best anchor for each true box best_anchor = np.argmax(iou, axis=-1) for t, n in enumerate(best_anchor): for l in range(num_layers): if n in anchor_mask[l]: i = np.floor(true_boxes[b,t,0]*grid_shapes[l][1]).astype('int32') j = np.floor(true_boxes[b,t,1]*grid_shapes[l][0]).astype('int32') k = anchor_mask[l].index(n) c = true_boxes[b,t, 4].astype('int32') y_true[l][b, j, i, k, 0:4] = true_boxes[b,t, 0:4] y_true[l][b, j, i, k, 4] = 1 y_true[l][b, j, i, k, 5+c] = 1 return y_true 该函数用于将真实盒子信息转化成训练所需要的格式。 参数： true_boxes: 多维数组，要求形状为 (batch_size,boxes_number,5) 由 get_random_data 生成 input_shape: 网络的输入尺寸应该是32的倍数，注意这里的尺寸是 高在前、宽在后 anchors: 锚框，形状为(k,2) k是锚框个数; 2 是(w,h) num_classes: 整型,总类别数 返回： y_true ：和 yolo3 网络输出的数据同型的数组，此时的 wxyh 已经换为相对路径 执行过程： 计算 YOLO3 网络输出的特征图数目，这里认为输出的特征图数目与锚框数目有关，经典 YOLO3 输出三个特征图，每个特征图 3 个锚框，共 9 个。tiny-YOLO 则输出 2 个特征图，每个特征图 3 个锚框，所以可由锚框数目求 YOLO3 输出的特征图数目，并分辨网络类型。其中： num_layers 表示输出特征图数目 anchor_mask 是锚框掩码，用于规定每个特征图用哪几个锚框 之后均以 YOLO3 输出三个特征图，每个特征图 3 个锚框 为例。 对于参数 true_boxes 和 input_shape 进行类型转换 求出每个盒子的中心点坐标 boxes_xy 与宽高信息 boxes_wh 这两个参数形状为 (batch_size,boxes_number,2) 对 true_boxes 中的位置和宽高进行修改，将其改为相对于原图的比例数值，称新的 true_boxes[..., 0:4] 为最小横坐标比例、最小纵坐标比例、最大横坐标比例、最大纵坐标比例 求 grid_shapes , grid_shapes 表示每个输出的特征图的两个方向各自分布多少网格。对于最终输出的特诊图而言，特征图上的一个点对应原图的一个区域，这个区域被称为网格，所以可以理解为这个变量表示的是输出的每个特征图的宽和高，经典 YOLO3 输出三个特征图，每个特征图大小别为原图宽高分别除 32、原图宽高分别除 16 、原图宽高分别除 8，最终 grid_shapes 的结构是 np 数组的列表： [np.array([f1_w,f1_h]),np.array([f2_w,f2_h]),np.array([f3_w,f3_h])] ，其中 f1_w,f1_h 表示最终输出的特征图里第一个特征图的宽和高。 创建 y_true ，此时只是先把 y_true 的结构定下来，其值用 0 填充，具体数值后面再填，y_true 的结构也是 np 数组的列表 ，它的结构为 [y_true_1,y_true_2,y_true_3] 。其中 y_true_* 是 np 数组，表示一个特征图中包含的真实信息，它的shape=(批数,特征图高,特征图宽,3,5+类别数) 之后的代码是针对每批数据而言，故之后的代码 假设处理下标为 b 的一批数据 对 b 批数据的对每个盒子对预设的锚框进行 IOU 运算，这里是假设中心点重合的 IOU，最终目的是求 best_anchor 。best_anchor 表示对于每个盒子而言，参数 anchors 里的哪个锚框和这个盒子重合度最高，它的 shape=(valid_boxes_number,) ，即长度为valid_boxes_number 的向量，其中 valid_boxes_number 为有效盒子数，有效盒子被定义为宽度大于 0 的盒子。 这里的实现有个细节，就是 anchors 被扩充维度成 shape=(1,k,2) 而wh被扩充维度为 shape=(boxes_number,1,2) 所以最终他们的运算结果由 broadcasting 变成 shape=(boxes_number,k,2) 之后是一个二层循环，分别表示遍历best_anchor，遍历每个输出特征图，变量 b,t,n,l 都是索引号，分别表示：第 b 号批 、第 t 号盒子 、第t 号盒子的最大重合 anchor 索引号为 n 、第 l 号特征图 如果第 t 号盒子的最大索引号 n，在第 l 号特征图的锚框掩码中，则说明，第l 号特征图内有锚框负责这个盒子，此时： 令 i 等于第 b 号批里第 t 号盒子的最大横坐标比例*第 l 号特征图的宽，并向下取整 令 j 等于第 b 号批里第 t 号盒子的最小纵坐标比例*第 l 号特征图的高，并向下取整 此时 j,i 表示第 l 号特征图中，负责这个物体的网格坐标。 令 k 等于第 n 号锚框在第 l 号特征的索引号，k 只能取 0,1,2 令 c 等于第 b 号批里第 t 号盒子的类别号 最后填充，第 l 号特征图中的，第 b 号批数据里，第 j 行 、第 i 列，第 k 号锚框，使其坐标信息等于 b 号批里第 t 号盒子的坐标信息、置信度等于 1、类别信息等于 c （即令 y_true[l][..,5+c] = 1 ) 上述过程把 y_true 该填的完，负责物体的锚框有对应值，不负责物体的锚框的值为 0，最后强调，y_true 是一个三元素列表，每个元素的 shape=(批数,特征图高,特征图宽,3,5+类别数) ，每个元素的元素可视为长度为 5+类别数 的特征向量，特征向量中的位置信息是相对于原图的最小横坐标比例、最小纵坐标比例、最大横坐标比例、最大纵坐标比例。 实现函数训练数据是由 data_generator_wrapper 生成： data_generator_wrapperdata_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes): n = len(annotation_lines) if n==0 or batch_size&lt;=0: return None return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes) 参数： annotation_lines：yolo3 标注文件的行数，也就是总训练样本数 batch_size ：批大小 input_shape ：二维元组，输入张量的形状，要求32的倍数，YOLO3 设置为 (416,416) anchors ：锚框列表，get_anchors 函数得到，形如[[w1,h1],[w2,h2]...] num_classes ：类别数目，由 len(get_classes(classes_path)) 得到 data_generator_wrapper 只是对参数进行简单的检验，最终数据由 data_generator 生成 data_generatordef data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes): '''data generator for fit_generator''' n = len(annotation_lines) i = 0 while True: image_data = [] box_data = [] for b in range(batch_size): if i==0: np.random.shuffle(annotation_lines) image, box = get_random_data(annotation_lines[i], input_shape, random=True) image_data.append(image) box_data.append(box) i = (i+1) % n image_data = np.array(image_data) box_data = np.array(box_data) y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes) yield [image_data, *y_true], np.zeros(batch_size) data_generator 是一个生成器，所以用 yield 替代 return ，这生成器直接作用于 model.fit_generator ，所以生成器生成的数据，就是每一epoch 训练的数据。 生成器的参数和 data_generator_wrapper 参数一致。 model.fit_generator 要求 data_generator 返回一个下面任意一个元组： a tuple (inputs, targets) a tuple (inputs, targets, sample_weights) 这里 data_generator 返回的是 (inputs, targets) 它的 inputs 是 [image_data, *y_true] , 它的 targets 是长为 batch_size 的全 0 向量。 执行过程： 统计样本总数 n = len(annotation_lines) , 设置计数变量 i = 0 通过 while-yield 结构设计生成器 每次生成数据的时候 , 执行 batch_size 次循环 如果 i==0 说明第一次生成数据 , 或者所有样本都已经被生成一遍了 , 此时用 np.random.shuffle(annotation_lines) 将样本打乱 通过 get_random_data(annotation_lines[i], input_shape, random=True) 获得第 i 个样本的图片信息和图片上的盒子信息 image, box 将图片信息和标注信息分布追加到image_data, box_data i = (i+1) % n 表示该读下一个样本了 将 image_data, box_data 包装为 np 数据 通过 preprocess_true_boxes(box_data, input_shape, anchors, num_classes) 由盒子信息获取 loss 函数所需的目标信息 y_true 返回 [image_data, *y_true], np.zeros(batch_size)","link":"/2020/04/04/YOLO3%20%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%20Keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"YOLO3 网络结构 Keras源码分析","text":"YOLO3 Keras 源码：https://github.com/qqwweee/keras-yolo3 网络结构网络结构图 图片来自：https://blog.csdn.net/leviopku/article/details/82660381 将 YOLO3 网络结构划分为多个层和层包，下面的各个层包的解释： conv：卷积层 DBL： Darknetconv2d_BN_Leaky，conv + BN + Leaky relu。 res*：resblock_body， * 是数字，表示内含res_unit数目，有res1，res2, … ,res8等等 res_unit ： 借鉴残差网络思想，将特征值 $\\pmb x$ 与经过两个 DBL 的净输入值 $\\pmb z$ 相加作为最终净输入。 操作： concat：张量拼接，即tf.concat，会使最后一个维度变长。 Darknetconv2d_BN_Leaky什么是 Darknetconv2d_BN_Leaky Darknetconv2d_BN_Leaky 是指在卷积层后接一个 BN 然后再通过 relu 函数，这个不是在 YOLO3 提出的，在 YOLO2 就采取在每个卷积层后进行 BN 的做法了。 YOLO3 keras 源码中的 Darknetconv2d_BN_Leaky 的特征图尺寸，要么等于原图尺寸，要么原图的 1/4 主要看 strides 参数怎么设定。 为什么使用 Darknetconv2d_BN_Leaky 接 BN 主要是为了解决梯度消失问题，同时因为 BN 可以把特诊图拉回均值为 0 ，方差为 1 的分布，所以也能对解决内部协变量位移问题起一定作用，保证对于深层网络而言，每批数据的分布大致相同。（当然均值和方差相同也不一定就是分布相同，所以作用有限） 从 YOLO1 其就在使用 Leaky relu 作为激活函数，Leaky relu 对比 relu 的好处就是，总是有梯度存在的，不至于让深层神经元在训练过程中因为梯度始终为 0 而出现不更新的情况。 如何实现 Darknetconv2d_BN_Leaky 在 \\yolo3\\model.py 文件中： @wraps(Conv2D)def DarknetConv2D(*args, **kwargs): \"\"\"Wrapper to set Darknet parameters for Convolution2D.\"\"\" darknet_conv_kwargs = {'kernel_regularizer': l2(5e-4)} #l2 是keras的l2正则化器 darknet_conv_kwargs['padding'] = 'valid' if kwargs.get('strides')==(2,2) else 'same' darknet_conv_kwargs.update(kwargs) return Conv2D(*args, **darknet_conv_kwargs)def DarknetConv2D_BN_Leaky(*args, **kwargs): \"\"\"Darknet Convolution2D followed by BatchNormalization and LeakyReLU.\"\"\" no_bias_kwargs = {'use_bias': False} no_bias_kwargs.update(kwargs) return compose( DarknetConv2D(*args, **no_bias_kwargs), BatchNormalization(), LeakyReLU(alpha=0.1)) 源码分析：l2(5e-4) :tf.keras.regularizers.l2( l=0.01) 参数: l: 浮点型，L2 正则项前的系数 l2(5e-4) 表示使用 L2 正则项，系数为 5e-4 Conv2D(*args, **darknet_conv_kwargs) :tf.keras.layers.Conv2D( filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs) 卷积层 参数 filters: 整数，卷积核数目，这个参数决定输出的特征图的深度。 kernel_size: 整数，或者整数列表，卷积核大小 strides: 由两个整数组成的整数或元组/列表，指定卷积沿高度和宽度的步幅。可以是单个整数，以指定所有空间维度的相同值。 padding: &quot;valid&quot; 或 &quot;same&quot; (不区分大小写).两个值代表窄卷积和等宽卷积 data_format: 一个字符串, channels_last (默认) 或者 channels_first 其中之一. 代表输入的张量中通道在哪个维度 channels_last 相当于输入的张量形如 (batch, height, width, channels) 而channels_first 相当于输入张量形如 (batch, channels, height, width). dilation_rate: 一个由2个整数组成的整数或元组/列表，指定用于扩张卷积（即空洞卷积）的扩张率。可以是单个整数，以指定所有空间维度的相同值。 activation: 指定激活函数，默认不指定 use_bias: 布尔型，是否使用偏置向量 kernel_initializer: 权重矩阵初始化器 bias_initializer: 偏置向量初始化器 kernel_regularizer: 权重矩阵正则化器 bias_regularizer: 偏置向量正则化器 activity_regularizer: 激活函数初始化器 kernel_constraint: 优化器更新权重后，用于权重的约束函数，要求函数接受一个张量且输出相同形状的张量。 bias_constraint: 优化器更新偏置后，用于偏置向量的约束函数，要求函数接受一个张量且输出相同形状的张量。 Conv2D(*args, **darknet_conv_kwargs) : 对 args 和 darknet_conv_kwargs 解包传参，即除了初始化器，参数都从外部传入。 DarknetConv2D_BN_Leaky(*args, **kwargs) :DBL 块，这个函数主要返回一个函数，该函数通过 compose 构造： compose( DarknetConv2D(*args, **no_bias_kwargs), BatchNormalization(), LeakyReLU(alpha=0.1)) compose 是定义在 \\yolo3\\utils.py 的函数 def compose(*funcs): \"\"\"Compose arbitrarily many functions, evaluated left to right. Reference: https://mathieularose.com/function-composition-in-python/ \"\"\" # return lambda x: reduce(lambda v, f: f(v), funcs, x) if funcs: return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs) else: raise ValueError('Composition of empty sequence not supported.') 其作用主要是通过functools.reduce() 实现可调用对象的连续调用 DarknetConv2D_BN_Leaky(*args, **kwargs) 实际上返回的是 LeakyReLU(alpha=0.1))(BatchNormalization()(DarknetConv2D(*args, **no_bias_kwargs)(Inputs))) 即输入张量经过 DarknetConv2D 后接 BatchNormalization 接 LeakyReLU 。 其中，BatchNormalization 和 LeakyReLU 分别： tf.keras.layers.BatchNormalization( axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None, renorm=False, renorm_clipping=None, renorm_momentum=0.99, fused=None, trainable=True, virtual_batch_size=None, adjustment=None, name=None, **kwargs) 部分参数： axis: 整数，应该被标准化的轴数（第几个维度） 经过卷积的数据应该是通道维度被标准化，所以在data_format=&quot;channels_first&quot; 的 Conv2D 后 , 设置 axis=1 。 momentum: 移动均值和移动方差的动量。 epsilon: 增加到方差的小的浮点数，以避免除以零。 center:如果为 True，把 beta 的偏移量加到标准化的张量上。 如果为 False， beta 被忽略。 scale: 如果为 True，乘以 gamma。 如果为 False，gamma 不使用。 当下一层为线性层（或者例如 nn.relu）， 这可以被禁用，因为缩放将由下一层完成。 tf.keras.layers.LeakyReLU( alpha=0.3, **kwargs) 该函数被解释为f(x) = alpha * x for x &lt; 0, f(x) = x for x &gt;= 0. resblock_body什么是 resblock_body 一个 resblock_body 包含多个 res_unit，res 是 (Residual Network， ResNet ) 的缩写 , 每个 res_unit 被定义为 Res(\\pmb Z)= \\pmb Z + DBL(DBL(\\pmb Z))其中的 DBL 都是不改变尺寸的，所以 $+$ 即按元素相加 为什么使用 resblock_body 根据通用近似定理，一个由神经网络构成的非线性单元有足够的能力来近似逼近原始目标函数或残差函数，但实际中后者更容易学习 [He等人，2016]。 参考论文： He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C] //Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778. 容易学习主要体现在 一方面，$h(z)=z+f(z)$ 在求 $z$ 的偏导时，有链式法则得 ： \\frac{\\partial E}{\\partial w} = \\cdots \\frac{\\partial h}{\\partial z}\\cdot \\frac{\\partial z}{\\partial w} \\cdots \\\\=\\cdots (1+\\frac{\\partial f}{\\partial z})\\cdot \\frac{\\partial z}{\\partial w} \\cdots相较于普通深度神经网络的梯度相乘，加入常数项 1 一定程度上能稳定梯度，使它不容易消失。让深层神经网络成为可能。 其次，残差网络可以将前基层的特征隔层传递给下面的层，让一些可能被忽视的特征有了再被提取的可能。 怎么实现 resblock_body 在 \\yolo3\\model.py 文件中： def resblock_body(x, num_filters, num_blocks): '''A series of resblocks starting with a downsampling Convolution2D''' # Darknet uses left and top padding instead of 'same' mode x = ZeroPadding2D(((1,0),(1,0)))(x) x = DarknetConv2D_BN_Leaky(num_filters, (3,3), strides=(2,2))(x) for i in range(num_blocks): y = compose( DarknetConv2D_BN_Leaky(num_filters//2, (1,1)), DarknetConv2D_BN_Leaky(num_filters, (3,3)))(x) x = Add()([x,y]) return x 源码分析resblock_body(x, num_filters, num_blocks)x 是输入张量，num_filters 是 DBL 卷积核数目，决定了最终输出的通道数目，num_blocks 指 res_unit 数目。 可以看到特征图经过一个 resblock_body 先要提取特征并缩小尺寸，随后开始经历 num_blocks 个残差单元，每个残差单元的 y 都要经过一个 1*1 卷积和 3*3 卷积 ZeroPadding2D(((1,0),(1,0)))(x)tf.keras.layers.ZeroPadding2D( padding=(1, 1), data_format=None, **kwargs) 2D 输入的零填充层（例如图像）。 该图层可以在图像张量的顶部、底部、左侧和右侧添加零表示的行和列。 参数 padding : 整数，或 2 个整数的元组，或 2 个整数的 2 个元组。 如果为整数：将对宽度和高度运用相同的对称填充。 如果为 2 个整数的元组： 如果为整数：: 解释为高度和高度的 2 个不同的对称裁剪值： (symmetric_height_pad, symmetric_width_pad)。 如果为 2 个整数的 2 个元组： 解释为 ((top_pad, bottom_pad), (left_pad, right_pad))。 data_format: 字符串， channels_last (默认) 或 channels_first 之一， 表示输入中维度的顺序。channels_last 对应输入尺寸为 (batch, height, width, channels)， channels_first 对应输入尺寸为 (batch, channels, height, width)。 输入尺寸 如果 data_format 为 &quot;channels_last&quot;， 输入 4D 张量，尺寸为 (batch, rows, cols, channels)。 如果 data_format 为 &quot;channels_first&quot;， 输入 4D 张量，尺寸为 (batch, channels, rows, cols)。 输出尺寸 如果 data_format 为 &quot;channels_last&quot;， 输出 4D 张量，尺寸为 (batch, padded_rows, padded_cols, channels)。 如果 data_format 为 &quot;channels_first&quot;， 输出 4D 张量，尺寸为 (batch, channels, padded_rows, padded_cols)。 ZeroPadding2D(((1,0),(1,0)))(x) 表示在 x 的上面和左边填充一列 0 x = ZeroPadding2D(((1,0),(1,0)))(x)x = DarknetConv2D_BN_Leaky(num_filters, (3,3), strides=(2,2))(x) 上边和左边填一列零后，用 3*3 卷积核，以 2 跨步，valid 模式，经过卷积输出的张量尺寸为（(h-3+1)/2向下取整，(w-1+3)/2向下取整），这样也起到了池化层的作用。 Add()([x,y])tf.keras.layers.Add( **kwargs) 该实例没有舒适化参数，直接实例化调用就行，调用时的输入参数是一个张量列表，要求列表内的张量必须形状相同，最后结果是对应元素相加。 Add()([x,y]) 是 y 是经过两个 DBL 的特征图 darknet_body什么是 darknet_body darknet_body 就是 YOLO3 的支柱网络，它是 “Darknet-53 ”去掉最后一层卷积层，Darknet-53 结构图如下： 为什么使用 darknet_body 因为 Darknet-53 比 Darknet-19 牛逼，Darknet-53 也是改良自 Darknet-19 ，主要改良是引入了残差结构和用卷积层代替池化层。 引入残差结构可能为了能提高网络深度。 使用卷积代替池化，可能是因为单纯的池化没有可学习参数，一般都是在池化前加上 1*1 卷积增加池化层的特征提取功能，Darknet-53 把 1*1 卷积与池化层合并成一个卷积了，减少了一层运算。 怎么实现 darknet_body 在 \\yolo3\\model.py 文件中： def darknet_body(x): '''Darknent body having 52 Convolution2D layers''' x = DarknetConv2D_BN_Leaky(32, (3,3))(x) x = resblock_body(x, 64, 1) x = resblock_body(x, 128, 2) x = resblock_body(x, 256, 8) x = resblock_body(x, 512, 8) x = resblock_body(x, 1024, 4) return x 相关层的源码上面已经解释过了，这里就照着结构图复现就完事了。 yolo_body什么是 yolo_body yolo_body 从 yolo3 的支柱网络 Darknet-52 手里接过输出的特征图，兵分三路，最终生成了 y1，y2，y3 三个输出，三个输出深度一直，唯一不同就是尺寸，前面的 Darknet-52 是特征提取，到这就开始根据前面提取的特征进行目标识别了。 为什么要分成三个输出，每个输出还要拼接前面的层 因为自从 YOLO2 开始，YOLO 网络开始借助 RPN 的思想，最终输出的特征图的每一个像素点，其实包含了一个感受野区域的特征信息，显然 13*13 的特征图，意味着感受野比较大，这样的特征图适合识别大物体；相反 52*52 的感受野较小，这样的特征图适合识别小物体，这也是相对于YOLO2 的改进，这样 YOLO3 就既能识别大物体，又能识别小物体了。 拼接之前层的输出，是从 YOLO2 就开始有的思想，叫细粒度分类，也是为了能保留小物体的特征。 怎么实现 yolo_body 在 \\yolo3\\model.py 文件中： def make_last_layers(x, num_filters, out_filters): '''6 Conv2D_BN_Leaky layers followed by a Conv2D_linear layer''' x = compose( DarknetConv2D_BN_Leaky(num_filters, (1,1)), DarknetConv2D_BN_Leaky(num_filters*2, (3,3)), DarknetConv2D_BN_Leaky(num_filters, (1,1)), DarknetConv2D_BN_Leaky(num_filters*2, (3,3)), DarknetConv2D_BN_Leaky(num_filters, (1,1)))(x) y = compose( DarknetConv2D_BN_Leaky(num_filters*2, (3,3)), DarknetConv2D(out_filters, (1,1)))(x) return x, ydef yolo_body(inputs, num_anchors, num_classes): \"\"\"Create YOLO_V3 model CNN body in Keras.\"\"\" darknet = Model(inputs, darknet_body(inputs)) x, y1 = make_last_layers(darknet.output, 512, num_anchors*(num_classes+5)) x = compose( DarknetConv2D_BN_Leaky(256, (1,1)), UpSampling2D(2))(x) x = Concatenate()([x,darknet.layers[152].output]) x, y2 = make_last_layers(x, 256, num_anchors*(num_classes+5)) x = compose( DarknetConv2D_BN_Leaky(128, (1,1)), UpSampling2D(2))(x) x = Concatenate()([x,darknet.layers[92].output]) x, y3 = make_last_layers(x, 128, num_anchors*(num_classes+5)) return Model(inputs, [y1,y2,y3]) 这里 darknet 和 yolo3 都是用的 keras 的 Functional API 构建的网络模型，Functional API 挺简单的而且还支持多输出。 源码讲解make_last_layers(darknet.output, 512, num_anchors*(num_classes+5))这个函数先把 yolo3 的输出分成两个：y1 和不是 y1，源码结构清晰，一目了然，其中参数arknet.output 是提取前面已经封装成模型的 darknet 的输出，把前一个模型的输出作为这个模型的输入， 以前后顺序衔接两个模型。512 是中间 DBL 的卷积和数目，num_anchors*(num_classes+5) 是最终生成的特征图的深度，5 是指 4 个坐标信息 + 置信度。 yolo_body(inputs, num_anchors, num_classes)构建 YOLO3 网络，通过 UpSampling2D(2))(x) 、 darknet.layers[] 、和 Concatenate() 实现特征图拼接 darknet 中间某层的输出的操作。 UpSampling2D(2))(x)tf.keras.layers.UpSampling2D( size=(2, 2), data_format=None, interpolation='nearest', **kwargs) 2D 输入的上采样层。 沿着数据的行和列分别重复 size[0] 和 size[1] 次。 参数 size: 整数，或 2 个整数的元组。 行和列的上采样因子。 data_format: 字符串， channels_last (默认) 或 channels_first 之一， 表示输入中维度的顺序。channels_last 对应输入尺寸为 (batch, height, width, channels)， channels_first 对应输入尺寸为 (batch, channels, height, width)。 interpolation: 字符串，nearest 或 bilinear 之一。 输入尺寸 如果 data_format 为 &quot;channels_last&quot;， 输入 4D 张量，尺寸为 (batch, rows, cols, channels)。 如果 data_format 为 &quot;channels_first&quot;， 输入 4D 张量，尺寸为 (batch, channels, rows, cols)。 输出尺寸 如果 data_format 为 &quot;channels_last&quot;， 输出 4D 张量，尺寸为 (batch, upsampled_rows, upsampled_cols, channels)。 如果 data_format 为 &quot;channels_first&quot;， 输出 4D 张量，尺寸为 (batch, channels, upsampled_rows, upsampled_cols)。 上采用就是下采样的逆操作，这里是每个元素重复4次变成一个 4*4 区域，使整个特征图尺寸扩大 4 倍（高扩 2 倍，宽扩 2 倍） darknet.layers[]layers 是 tf.keras.Model 类的属性，可以通过 [] 提取特定的层，也可以用Model.get_layer() 方法替代。 x = Concatenate()([x,darknet.layers[152].output])tf.keras.layers.Concatenate( axis=-1, **kwargs) 连接一个输入张量的列表。 它接受一个张量的列表， 除了连接轴之外，其他的尺寸都必须相同， 然后返回一个由所有输入张量连接起来的输出张量。 参数 axis: 连接的轴。 \\kwargs: 层关键字参数。 输入 张量列表，要求除了连接轴之外，其他的尺寸都必须相同 注意区分于 Add() 一个是对应元素相加，一个把一个轴上的数据前后连接在一起。 Tiny YOLO 的网络结构通过对前面逐层代码的学习，Tiny YOLO 的结构一目了然，Tiny YOLO 缩减了 Darknet-52，并且只输出 y1 和 y2，它有更快的收敛速度，但是相对的精度没有 YOLO 高，它的源码在 \\yolo3\\model.py 文件中： def tiny_yolo_body(inputs, num_anchors, num_classes): '''Create Tiny YOLO_v3 model CNN body in keras.''' x1 = compose( DarknetConv2D_BN_Leaky(16, (3,3)), MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'), DarknetConv2D_BN_Leaky(32, (3,3)), MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'), DarknetConv2D_BN_Leaky(64, (3,3)), MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'), DarknetConv2D_BN_Leaky(128, (3,3)), MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'), DarknetConv2D_BN_Leaky(256, (3,3)))(inputs) x2 = compose( MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'), DarknetConv2D_BN_Leaky(512, (3,3)), MaxPooling2D(pool_size=(2,2), strides=(1,1), padding='same'), DarknetConv2D_BN_Leaky(1024, (3,3)), DarknetConv2D_BN_Leaky(256, (1,1)))(x1) y1 = compose( DarknetConv2D_BN_Leaky(512, (3,3)), DarknetConv2D(num_anchors*(num_classes+5), (1,1)))(x2) x2 = compose( DarknetConv2D_BN_Leaky(128, (1,1)), UpSampling2D(2))(x2) y2 = compose( Concatenate(), DarknetConv2D_BN_Leaky(256, (3,3)), DarknetConv2D(num_anchors*(num_classes+5), (1,1)))([x2,x1]) return Model(inputs, [y1,y2])","link":"/2020/04/04/YOLO3%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%20Keras%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}],"tags":[{"name":"YOLO3","slug":"YOLO3","link":"/tags/YOLO3/"},{"name":"Keras","slug":"Keras","link":"/tags/Keras/"}],"categories":[{"name":"YOLO3 源码分析","slug":"YOLO3-源码分析","link":"/categories/YOLO3-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}]}